<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>dxer</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://dxer.github.io/"/>
  <updated>2016-05-12T07:55:59.624Z</updated>
  <id>http://dxer.github.io/</id>
  
  <author>
    <name>dxer</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HBase性能优化方法总结</title>
    <link href="http://dxer.github.io/2016/04/01/hbase-optimize/"/>
    <id>http://dxer.github.io/2016/04/01/hbase-optimize/</id>
    <published>2016-04-01T07:25:38.000Z</published>
    <updated>2016-05-12T07:55:59.624Z</updated>
    
    <content type="html">&lt;p&gt;本文主要是从HBase应用程序设计与开发的角度，总结几种常用的性能优化方法。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Auto-Flash&quot;&gt;&lt;a href=&quot;#Auto-Flash&quot; class=&quot;headerlink&quot; title=&quot;Auto Flash&quot;&gt;&lt;/a&gt;Auto Flash&lt;/h3&gt;&lt;p&gt;通过调用&lt;code&gt;HTable.setAutoFlushTo(false)&lt;/code&gt;方法可以将HTable写客户福安的自动flush关闭，这样可以批量写入数据到HBase，而不是有一条put就执行一次更新，只有当put填满客户端写缓存的时候，才会向HBase服务端发起写请求。默认情况下auto flush是开启的。&lt;/p&gt;
&lt;h3 id=&quot;Write-Buffer&quot;&gt;&lt;a href=&quot;#Write-Buffer&quot; class=&quot;headerlink&quot; title=&quot;Write Buffer&quot;&gt;&lt;/a&gt;Write Buffer&lt;/h3&gt;&lt;p&gt;通过调用&lt;code&gt;HTable.setWriteBufferSize(writeBufferSize)&lt;/code&gt;方法可以设置HTable客户端的写buffer大小，如果新设置的buffer小于当前写buffer中的数据时，buffer将会被flush到服务端。其中，writeBufferSize的单位是byte字节数，可以根基实际写入数据量的多少来设置该值。&lt;/p&gt;
&lt;h3 id=&quot;WAL-Flag&quot;&gt;&lt;a href=&quot;#WAL-Flag&quot; class=&quot;headerlink&quot; title=&quot;WAL Flag&quot;&gt;&lt;/a&gt;WAL Flag&lt;/h3&gt;&lt;p&gt;在HBase中，客户端向集群中的RegionServer提交数据时（Put/Delete操作），首先会写到WAL（Write Ahead Log）日志，即HLog，一个RegionServer上的所有Region共享一个HLog，只有当WAL日志写成功后，再接着写MemStore，然后客户端被通知提交数据成功，如果写WAL日志失败，客户端被告知提交失败，这样做的好处是可以做到RegionServer宕机后的数据恢复。&lt;br&gt;对于不太重要的数据，可以在Put/Delete操作时，通过调用&lt;code&gt;Put.setWriteToWAL(false)&lt;/code&gt;或&lt;code&gt;Delete.setWriteToWAL(false)&lt;/code&gt;函数，放弃写WAL日志，以提高数据写入的性能。&lt;/p&gt;
&lt;p&gt;注：如果关闭WAL日志，一旦RegionServer宕机，Put/Delete的数据将会无法根据WAL日志进行恢复。&lt;/p&gt;
&lt;h3 id=&quot;Compression-压缩&quot;&gt;&lt;a href=&quot;#Compression-压缩&quot; class=&quot;headerlink&quot; title=&quot;Compression 压缩&quot;&gt;&lt;/a&gt;Compression 压缩&lt;/h3&gt;&lt;p&gt;数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，性价比最高。&lt;br&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;HColumnDescriptor hcd = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; HColumnDescriptor(familyName);   &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hcd.setCompressionType(Algorithm.SNAPPY);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&quot;批量写&quot;&gt;&lt;a href=&quot;#批量写&quot; class=&quot;headerlink&quot; title=&quot;批量写&quot;&gt;&lt;/a&gt;批量写&lt;/h3&gt;&lt;p&gt;通过调用&lt;code&gt;HTable.put(Put)&lt;/code&gt;方法可以将一个指定的row key记录写入HBase，同样HBase提供了另一个方法：通过调用&lt;code&gt;HTable.put(List&amp;lt;Put&amp;gt;)&lt;/code&gt;方法可以将指定的row key列表，批量写入多行记录，这样做的好处是批量执行，只需要一次网络I/O开销，这对于对数据实时性要求高，网络传输RTT高的情景下可能带来明显的性能提升。&lt;/p&gt;
&lt;h3 id=&quot;多线程并发写&quot;&gt;&lt;a href=&quot;#多线程并发写&quot; class=&quot;headerlink&quot; title=&quot;多线程并发写&quot;&gt;&lt;/a&gt;多线程并发写&lt;/h3&gt;&lt;p&gt;在客户端开启多个 HTable 写线程，每个写线程负责一个 HTable 对象的 flush 操作，这样结合定时 flush 和写 buffer（writeBufferSize），可以既保证在数据量小的时候，数据可以在较短时间内被 flush（如1秒内），同时又保证在数据量大的时候，写 buffer 一满就及时进行 flush。&lt;/p&gt;
&lt;h3 id=&quot;批量读&quot;&gt;&lt;a href=&quot;#批量读&quot; class=&quot;headerlink&quot; title=&quot;批量读&quot;&gt;&lt;/a&gt;批量读&lt;/h3&gt;&lt;p&gt;通过调用 &lt;code&gt;HTable.get(Get)&lt;/code&gt; 方法可以根据一个指定的 row key 获取一行记录，同样 HBase 提供了另一个方法：通过调用 &lt;code&gt;HTable.get(List)&lt;/code&gt; 方法可以根据一个指定的 row key 列表，批量获取多行记录，这样做的好处是批量执行，只需要一次网络 I/O 开销，这对于对数据实时性要求高而且网络传输 RTT 高的情景下可能带来明显的性能提升。&lt;/p&gt;
&lt;h3 id=&quot;缓存查询结果&quot;&gt;&lt;a href=&quot;#缓存查询结果&quot; class=&quot;headerlink&quot; title=&quot;缓存查询结果&quot;&gt;&lt;/a&gt;缓存查询结果&lt;/h3&gt;&lt;p&gt;对于频繁查询 HBase 的应用场景，可以考虑在应用程序中做缓存，当有新的查询请求时，首先在缓存中查找，如果存在则直接返回，不再查询 HBase；否则对 HBase 发起读请求查询，然后在应用程序中将查询结果缓存起来。至于缓存的替换策略，可以考虑 LRU 等常用的策略。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;HBase数据表优化&quot;&gt;&lt;a href=&quot;#HBase数据表优化&quot; class=&quot;headerlink&quot; title=&quot;HBase数据表优化&quot;&gt;&lt;/a&gt;HBase数据表优化&lt;/h2&gt;&lt;h3 id=&quot;预分区&quot;&gt;&lt;a href=&quot;#预分区&quot; class=&quot;headerlink&quot; title=&quot;预分区&quot;&gt;&lt;/a&gt;预分区&lt;/h3&gt;&lt;p&gt;默认情况下，在创建HBase表的时候会自动创建一个Region分区，当导入数据的时候，所有的HBase客户端都向Region写数据，知道这个Region足够大才进行切分，一种可以加快批量写入速度的方法是通过预先创建一些空的Regions，这样当数据写入HBase的时候，会按照Region分区情况，在进群内做数据的负载均衡。&lt;/p&gt;
&lt;h3 id=&quot;Rowkey优化&quot;&gt;&lt;a href=&quot;#Rowkey优化&quot; class=&quot;headerlink&quot; title=&quot;Rowkey优化&quot;&gt;&lt;/a&gt;Rowkey优化&lt;/h3&gt;&lt;p&gt;rowkey是按照字典存储，因此设置rowkey时，要充分利用排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放到一块。&lt;br&gt;rowkey若是递增生成的，建议不要使用正序直接写入，可以使用字符串反转方式写入，使得rowkey大致均衡分布，这样设计的好处是能将RegionServer的负载均衡，否则容易产生所有新数据都在集中在一个RegionServer上堆积的现象，这一点还可以结合table的与分区设计。&lt;/p&gt;
&lt;h3 id=&quot;减少Column-Family数量&quot;&gt;&lt;a href=&quot;#减少Column-Family数量&quot; class=&quot;headerlink&quot; title=&quot;减少Column Family数量&quot;&gt;&lt;/a&gt;减少Column Family数量&lt;/h3&gt;&lt;p&gt;不要在一张表中定义太多的column family。目前HBase并不能很好的处理超过2-3个column family的表，因为某个column family在flush的时候，它临近的column family也会因关联效应被触发flush，最终导致系统产生更过的I/O;&lt;/p&gt;
&lt;h3 id=&quot;设置最大版本数&quot;&gt;&lt;a href=&quot;#设置最大版本数&quot; class=&quot;headerlink&quot; title=&quot;设置最大版本数&quot;&gt;&lt;/a&gt;设置最大版本数&lt;/h3&gt;&lt;p&gt;创建表的时候，可以通过 &lt;code&gt;HColumnDescriptor.setMaxVersions(int maxVersions)&lt;/code&gt; 设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置 setMaxVersions(1)。&lt;/p&gt;
&lt;h3 id=&quot;缓存策略（setCaching）&quot;&gt;&lt;a href=&quot;#缓存策略（setCaching）&quot; class=&quot;headerlink&quot; title=&quot;缓存策略（setCaching）&quot;&gt;&lt;/a&gt;缓存策略（setCaching）&lt;/h3&gt;&lt;p&gt;创建表的时候，可以通过&lt;code&gt;HColumnDEscriptor.setInMemory(true)&lt;/code&gt;将表放到RegionServer的缓存中，保证在读取的时候被cache命中。&lt;/p&gt;
&lt;h3 id=&quot;设置存储生命期&quot;&gt;&lt;a href=&quot;#设置存储生命期&quot; class=&quot;headerlink&quot; title=&quot;设置存储生命期&quot;&gt;&lt;/a&gt;设置存储生命期&lt;/h3&gt;&lt;p&gt;创建表的时候，可以通过&lt;code&gt;HColumnDescriptor.setTimeToLive(int timeToLive)&lt;/code&gt;设置表中数据的存储生命周期，过期数据将自动被删除&lt;/p&gt;
&lt;h3 id=&quot;磁盘配置&quot;&gt;&lt;a href=&quot;#磁盘配置&quot; class=&quot;headerlink&quot; title=&quot;磁盘配置&quot;&gt;&lt;/a&gt;磁盘配置&lt;/h3&gt;&lt;p&gt;每台RegionServer管理10-1000个Regions。每个Region在1-2G，则每台server最少要10G，最大要1000*2G=2TB，考虑3备份，需要6TB。方案1是3块2TB磁盘，2是12块500G磁盘，带宽足够时，后者能提供更大的吞吐率，更细力度的冗余备份，更快速的单盘故障恢复。&lt;/p&gt;
&lt;h3 id=&quot;分配何时的内存给RegionServer&quot;&gt;&lt;a href=&quot;#分配何时的内存给RegionServer&quot; class=&quot;headerlink&quot; title=&quot;分配何时的内存给RegionServer&quot;&gt;&lt;/a&gt;分配何时的内存给RegionServer&lt;/h3&gt;&lt;p&gt;在不影响其他服务的情况下，越大越好。在HBase的conf目录下的hbase-env.sh的最后添加&lt;code&gt;export HBASE_REGIONSERVER_OPTS=&amp;quot;- Xmx16000m $HBASE_REGIONSERVER_OPTS&amp;quot;&lt;/code&gt;&lt;br&gt;其中16000m为分配给REgionServer的内存大小。&lt;/p&gt;
&lt;h3 id=&quot;写数据的备份数&quot;&gt;&lt;a href=&quot;#写数据的备份数&quot; class=&quot;headerlink&quot; title=&quot;写数据的备份数&quot;&gt;&lt;/a&gt;写数据的备份数&lt;/h3&gt;&lt;p&gt;备份数与读性能是成正比，与写性能成反比，且备份数影响高可用性。有两种配置方式，一种是将hdfs-site.xml拷贝到hbase的conf目录下，然后在其中添加或修改配置项&lt;code&gt;dfs.replication&lt;/code&gt;的值为要设置的备份数，这种修改所有的HBase用户都生效。另一种方式是改写HBase代码，让HBase支持针对列族设置备份数，在创建表时，设置列族备份数，默认为3，此种备份数支队设置的列族生效。&lt;/p&gt;
&lt;h3 id=&quot;客户端一次从服务器拉取的数量&quot;&gt;&lt;a href=&quot;#客户端一次从服务器拉取的数量&quot; class=&quot;headerlink&quot; title=&quot;客户端一次从服务器拉取的数量&quot;&gt;&lt;/a&gt;客户端一次从服务器拉取的数量&lt;/h3&gt;&lt;p&gt;通过配置一次拉取较大的数据量可以减少客户端获取数据的时间，但是他会占用客户端的内存，有三个地方可以进行配置&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在HBase的conf配置文件中进行配置&lt;code&gt;hbase.client.scanner.caching&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;通过调用&lt;code&gt;HTble.setScannerCaching(int scannerCaching)&lt;/code&gt;进行配置；&lt;/li&gt;
&lt;li&gt;通过调用&lt;code&gt;Sacn.setCaching(int caching)&lt;/code&gt;进行配置，三者的优先级越来越高。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;客户端拉取的时候指定列族&quot;&gt;&lt;a href=&quot;#客户端拉取的时候指定列族&quot; class=&quot;headerlink&quot; title=&quot;客户端拉取的时候指定列族&quot;&gt;&lt;/a&gt;客户端拉取的时候指定列族&lt;/h3&gt;&lt;p&gt;scan是指定需要column family，可以减少网络传输数据量，否则默认scan操作会返回整行所有column family的数据&lt;/p&gt;
&lt;h3 id=&quot;拉取完数据之后关闭ResultScanner&quot;&gt;&lt;a href=&quot;#拉取完数据之后关闭ResultScanner&quot; class=&quot;headerlink&quot; title=&quot;拉取完数据之后关闭ResultScanner&quot;&gt;&lt;/a&gt;拉取完数据之后关闭ResultScanner&lt;/h3&gt;&lt;p&gt;通过 scan 取完数据后，记得要关闭 ResultScanner，否则 RegionServer 可能会出现问题（对应的 Server 资源无法释放）。&lt;/p&gt;
&lt;h3 id=&quot;RegionServer的请求处理IO线程数&quot;&gt;&lt;a href=&quot;#RegionServer的请求处理IO线程数&quot; class=&quot;headerlink&quot; title=&quot;RegionServer的请求处理IO线程数&quot;&gt;&lt;/a&gt;RegionServer的请求处理IO线程数&lt;/h3&gt;&lt;p&gt;较少的IO线程适用于处理单次请求内存消耗较高的Big Put场景（大容量单词Put或设置了较大cache的scan，均数据Big Put）或RegionServer的内存比较紧张的场景。&lt;/p&gt;
&lt;p&gt;较多的IO线程，适用于单次请求内存消耗低，TPS要求（每次事务处理量）非常高的场景。这只该值的时候，以监控内存为主要参考&lt;/p&gt;
&lt;p&gt;在hbase-site.xml配置文件中配置项为&lt;code&gt;hbase.regionserver.handle.count&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;Region大小设置&quot;&gt;&lt;a href=&quot;#Region大小设置&quot; class=&quot;headerlink&quot; title=&quot;Region大小设置&quot;&gt;&lt;/a&gt;Region大小设置&lt;/h3&gt;&lt;p&gt;配置项&lt;code&gt;hbase.hregion.max.filesize&lt;/code&gt;，所属配置文件为hbase-site.xml，默认大小是256m。&lt;/p&gt;
&lt;p&gt;在当前RegionServer上单个Region的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的Region。小Region对split和compaction友好，因为拆分Region或compact小Region里的StoreFile速度非常快，内存占用低。缺点是split和compaction会很频繁，特别是数量较多的小Region不同的split，compaction，会导致集群响应时间波动很大，Region数量太多不仅给管理上带来麻烦，设置会引起一些HBase个bug。一般 512M 以下的都算小 Region。大 Region 则不太适合经常 split 和 compaction，因为做一次 compact 和 split 会产生较长时间的停顿，对应用的读写性能冲击非常大。&lt;/p&gt;
&lt;p&gt;此外，大 Region 意味着较大的 StoreFile，compaction 时对内存也是一个挑战。如果你的应用场景中，某个时间点的访问量较低，那么在此时做 compact 和 split，既能顺利完成 split 和 compaction，又能保证绝大多数时间平稳的读写性能。compaction 是无法避免的，split 可以从自动调整为手动。只要通过将这个参数值调大到某个很难达到的值，比如 100G，就可以间接禁用自动 split(RegionServer 不会对未到达 100G 的 Region 做 split)。再配合 RegionSplitter 这个工具，在需要 split 时，手动 split。手动 split 在灵活性和稳定性上比起自动 split 要高很多，而且管理成本增加不多，比较推荐 online 实时系统使用。内存方面，小 Region 在设置 memstore 的大小值上比较灵活，大 Region 则过大过小都不行，过大会导致 flush 时 app 的 IO wait 增高，过小则因 StoreFile 过多影响读性能。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要是从HBase应用程序设计与开发的角度，总结几种常用的性能优化方法。&lt;br&gt;
    
    </summary>
    
    
      <category term="hbase" scheme="http://dxer.github.io/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>HBase简介</title>
    <link href="http://dxer.github.io/2016/03/19/hbase%E7%AE%80%E4%BB%8B/"/>
    <id>http://dxer.github.io/2016/03/19/hbase简介/</id>
    <published>2016-03-19T03:21:38.000Z</published>
    <updated>2016-05-12T08:25:38.218Z</updated>
    
    <content type="html">&lt;p&gt;HBase是一个分布式的、面向列的开源数据库。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;HBase表的特点&quot;&gt;&lt;a href=&quot;#HBase表的特点&quot; class=&quot;headerlink&quot; title=&quot;HBase表的特点&quot;&gt;&lt;/a&gt;HBase表的特点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;大：一个表可以有有数以十亿行，上百万列&lt;/li&gt;
&lt;li&gt;面向列：面向列（族）的存储和权限访问，列（族）独立索引&lt;/li&gt;
&lt;li&gt;稀疏：对于未空（null）的列，并不占用存储空间，因此表可以设计的非常稀疏&lt;/li&gt;
&lt;li&gt;数据类型单一：HBase中的数据类型都是字符串（string）&lt;/li&gt;
&lt;li&gt;无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态增加，同一张表中不同的行可以截然不同的列&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;HBase和关系数据库区别&quot;&gt;&lt;a href=&quot;#HBase和关系数据库区别&quot; class=&quot;headerlink&quot; title=&quot;HBase和关系数据库区别&quot;&gt;&lt;/a&gt;HBase和关系数据库区别&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;数据库类型：HBase中的数据类型都是字符串类型（string）&lt;/li&gt;
&lt;li&gt;数据操作：HBase只有普通的增删改查等操作，没有表之间的关联查询&lt;/li&gt;
&lt;li&gt;存储模式：HBase是基于列式存储模式，而RDBMS是基于行式存储的&lt;/li&gt;
&lt;li&gt;应用场景：HBase适合存储大量数据，查询效率极高&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;HBase以表的形式存储数据。表有行和列组成。列划分为若干个列族(column family)&lt;/p&gt;
&lt;h3 id=&quot;RowKey&quot;&gt;&lt;a href=&quot;#RowKey&quot; class=&quot;headerlink&quot; title=&quot;RowKey&quot;&gt;&lt;/a&gt;RowKey&lt;/h3&gt;&lt;p&gt;用来检索记录的主键&lt;br&gt;主键为任意字符串，最大长度为64kb，按字典顺序存储，在HBase内部保存为字节数组&lt;/p&gt;
&lt;p&gt;Rowkey是以字典顺序从大到小排序&lt;br&gt;Rowkey尽量散列设计，保证所有的数据不是在一个Region上，从而避免读写的时候负载会集中在个别Region上。&lt;br&gt;Rowkey的长度尽量短，如果太长存储开销会增加，影响存储效率，Rowkey字段过长，会导致内存的利用率降低，进而降低索引的命中率&lt;/p&gt;
&lt;h6 id=&quot;常见Rowkey设计方法：&quot;&gt;&lt;a href=&quot;#常见Rowkey设计方法：&quot; class=&quot;headerlink&quot; title=&quot;常见Rowkey设计方法：&quot;&gt;&lt;/a&gt;常见Rowkey设计方法：&lt;/h6&gt;&lt;ul&gt;
&lt;li&gt;反转userId，将userId字符串反转后存储&lt;/li&gt;
&lt;li&gt;散列userId，对userId进行散列&lt;/li&gt;
&lt;li&gt;userId取模后进行MD5，区前6位作为前缀加入到userId前面&lt;/li&gt;
&lt;li&gt;时间使用long型来表示&lt;/li&gt;
&lt;li&gt;尽量使用编码压缩&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;访问HBase表中的行，只有三种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过rowkey&lt;/li&gt;
&lt;li&gt;通过rowkey的range&lt;/li&gt;
&lt;li&gt;全表扫描&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;列族（Column-Family）&quot;&gt;&lt;a href=&quot;#列族（Column-Family）&quot; class=&quot;headerlink&quot; title=&quot;列族（Column Family）&quot;&gt;&lt;/a&gt;列族（Column Family）&lt;/h3&gt;&lt;p&gt;列族在创建表的时候声明，一个列族可以包含多个列，列中的数据都是以二进制形式存在，没有数据类型&lt;br&gt;列族是一些列的集合&lt;br&gt;一个列族所有成员是有着相同的前缀。用”:”来分割列族和列名&lt;/p&gt;
&lt;h3 id=&quot;列（Column）&quot;&gt;&lt;a href=&quot;#列（Column）&quot; class=&quot;headerlink&quot; title=&quot;列（Column）&quot;&gt;&lt;/a&gt;列（Column）&lt;/h3&gt;&lt;p&gt;属于某一个column family，columnfamily:columnName，每条记录可动态添加&lt;/p&gt;
&lt;h3 id=&quot;时间戳和存储单元（TimeStamp-and-Cell）&quot;&gt;&lt;a href=&quot;#时间戳和存储单元（TimeStamp-and-Cell）&quot; class=&quot;headerlink&quot; title=&quot;时间戳和存储单元（TimeStamp and Cell）&quot;&gt;&lt;/a&gt;时间戳和存储单元（TimeStamp and Cell）&lt;/h3&gt;&lt;p&gt;HBase中通过row和columns确定的唯一个存储单元成为cell，每个cell都保存同一份数据的多个版本&lt;br&gt;在写入数据时，时间戳可以又HBase自动赋值（当前系统时间精确到毫秒），也阔以显示赋值&lt;br&gt;每个cell中，不同版本的数据按照时间的倒叙排序&lt;br&gt;{row，Column，version}元组就是HBase中的一个cell&lt;/p&gt;
&lt;h2 id=&quot;HBase物理模型&quot;&gt;&lt;a href=&quot;#HBase物理模型&quot; class=&quot;headerlink&quot; title=&quot;HBase物理模型&quot;&gt;&lt;/a&gt;HBase物理模型&lt;/h2&gt;&lt;p&gt;HBase存储细节&lt;br&gt;每个列族存储在HDFS上的一个单独文件夹中&lt;br&gt;Key和Version number会在每个列族中存储一份&lt;br&gt;空值不会被保存&lt;br&gt;HBase 为每个值维护了多级索引，即：&lt;key, column=&quot;&quot; family,=&quot;&quot; name,=&quot;&quot; timestamp=&quot;&quot;&gt;&lt;/key,&gt;&lt;/p&gt;
&lt;p&gt;物理存储：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Table中所有行都按照row key的字典序排列；&lt;/li&gt;
&lt;li&gt;Table在行的方向上分割为多个Region；&lt;/li&gt;
&lt;li&gt;Region按大小分割的，每个表开始只有一个region，随着数据增多，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region，之后会有越来越多的region；&lt;/li&gt;
&lt;li&gt;Region是Hbase中分布式存储和负载均衡的最小单元，不同Region分布到不同RegionServer上。&lt;/li&gt;
&lt;li&gt;Region虽然是分布式存储的最小单元，但并不是存储的最小单元。Region由一个或者多个Store组成，每个store保存一个columns family；每个Strore又由一个memStore和0至多个StoreFile组成，StoreFile包含HFile；memStore存储在内存中，StoreFile存储在HDFS上。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;HBase架构与基本组件&quot;&gt;&lt;a href=&quot;#HBase架构与基本组件&quot; class=&quot;headerlink&quot; title=&quot;HBase架构与基本组件&quot;&gt;&lt;/a&gt;HBase架构与基本组件&lt;/h2&gt;&lt;h3 id=&quot;Client&quot;&gt;&lt;a href=&quot;#Client&quot; class=&quot;headerlink&quot; title=&quot;Client&quot;&gt;&lt;/a&gt;Client&lt;/h3&gt;&lt;p&gt;整个HBase集群的入口&lt;br&gt;使用HBase RPC机制与HMaster和HRegionServer通信&lt;br&gt;与HMaster通信进行管理类的操作&lt;br&gt;与HRegionServer通信进行读写类操作&lt;br&gt;包含访问HBase的接口，并维护cache来加快对HBase的访问，与HRegionServer交互&lt;/p&gt;
&lt;h3 id=&quot;ZooKeeper程序协调服务&quot;&gt;&lt;a href=&quot;#ZooKeeper程序协调服务&quot; class=&quot;headerlink&quot; title=&quot;ZooKeeper程序协调服务&quot;&gt;&lt;/a&gt;ZooKeeper程序协调服务&lt;/h3&gt;&lt;p&gt;保证任何时候，集群中只有一个Master（HA）&lt;br&gt;存储所有Region的寻址入口&lt;br&gt;实时监控Region server的上线和下线信息。并实时通知HMaster&lt;br&gt;存储HBase的schema和table元数据&lt;/p&gt;
&lt;h3 id=&quot;HBase主节点HMaster&quot;&gt;&lt;a href=&quot;#HBase主节点HMaster&quot; class=&quot;headerlink&quot; title=&quot;HBase主节点HMaster&quot;&gt;&lt;/a&gt;HBase主节点HMaster&lt;/h3&gt;&lt;p&gt;管理用户对Table的增删改查操作（表操作）&lt;br&gt;管理HRegionServer的负载均衡，调整Region分布&lt;br&gt;在Region split后，负责新Region的分配&lt;br&gt;在HRegionServer停机后，负责将失效的HRegionServer上的Region迁移&lt;br&gt;HMaster失效仅会导致所有元数据无法被修改，但是表的数据读写还是可以正常进行&lt;/p&gt;
&lt;h3 id=&quot;HRegionServer节点&quot;&gt;&lt;a href=&quot;#HRegionServer节点&quot; class=&quot;headerlink&quot; title=&quot;HRegionServer节点&quot;&gt;&lt;/a&gt;HRegionServer节点&lt;/h3&gt;&lt;p&gt;维护HRegion并往HDFS中写数据&lt;br&gt;当表的大小超过设置值时，split HRegion&lt;br&gt;在HRegionServer停机后，负责失效HRegionServer上的HRegion迁移&lt;/p&gt;
&lt;h2 id=&quot;HBase与Zookeeper&quot;&gt;&lt;a href=&quot;#HBase与Zookeeper&quot; class=&quot;headerlink&quot; title=&quot;HBase与Zookeeper&quot;&gt;&lt;/a&gt;HBase与Zookeeper&lt;/h2&gt;&lt;p&gt;HBase元数据存储在Zookeeper中&lt;br&gt;默认情况下,HBase管理Zookeeper实例，比如，启动或者停止Zookeeper&lt;br&gt;Zookeeper解决HBase单点故障问题&lt;br&gt;HMaster与HRegionServer启动时会向Zookeeper注册&lt;/p&gt;
&lt;h2 id=&quot;WAL&quot;&gt;&lt;a href=&quot;#WAL&quot; class=&quot;headerlink&quot; title=&quot;WAL&quot;&gt;&lt;/a&gt;WAL&lt;/h2&gt;&lt;p&gt;WAL是Regionserver在处理插入和删除的过程中用来记录操作内容的一种日志&lt;/p&gt;
&lt;p&gt;一个表由一个region或者多个region组成，region由regionserver进行管理&lt;br&gt;每个region包含memstore和storeFile，memstore存储在内存中，storeFile存储在磁盘中&lt;/p&gt;
&lt;h2 id=&quot;HBase在HDFS中存储&quot;&gt;&lt;a href=&quot;#HBase在HDFS中存储&quot; class=&quot;headerlink&quot; title=&quot;HBase在HDFS中存储&quot;&gt;&lt;/a&gt;HBase在HDFS中存储&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/hbase/.tmp&lt;/code&gt;：临时目录，当对表做创建和删除的时候，会将表move到该目录，然后进行操作&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/hbase/data&lt;/code&gt;：核心目录，存储HBase表的数据&lt;br&gt;默认情况下，目录下有两个目录&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/hbase/data/default&lt;/code&gt;:&lt;br&gt;在用户创建表的时候，没有指定namespace时，表就创建在此目录下&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/hbase/data/hbase&lt;/code&gt;：系统内部创建的表，.META.表（region的详细信息）和namespace表（namespace信息）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/hbase/hbase.id&lt;/code&gt;：存储的是集群的唯一cluster id（uuid）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/hbase/hbase.version&lt;/code&gt;：集群的版本号&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/hbase/oldWALs&lt;/code&gt;: 对应0.94.x版本中.oldlogs目录&lt;br&gt;当/hbase/WALs目录中的logs没有之后，会将这些logs移动到此目录下，HMaster会定期清理&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;HBase使用场景&quot;&gt;&lt;a href=&quot;#HBase使用场景&quot; class=&quot;headerlink&quot; title=&quot;HBase使用场景&quot;&gt;&lt;/a&gt;HBase使用场景&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;大数据量存储，大数据量高并发操作&lt;/li&gt;
&lt;li&gt;需要对数据随机读写操作&lt;/li&gt;
&lt;li&gt;读写访问均是非常简单的操作&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;HBase与HDFS对比&quot;&gt;&lt;a href=&quot;#HBase与HDFS对比&quot; class=&quot;headerlink&quot; title=&quot;HBase与HDFS对比&quot;&gt;&lt;/a&gt;HBase与HDFS对比&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;两者都具有良好的容错性和扩展性，都可以扩展到成百上千个节点；&lt;/li&gt;
&lt;li&gt;HDFS适合批处理场景，不支持数据随机查找，不适合增量数据处理，不支持数据更新&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;HBase是一个分布式的、面向列的开源数据库。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。&lt;br&gt;
    
    </summary>
    
    
      <category term="hbase" scheme="http://dxer.github.io/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>HBase入门</title>
    <link href="http://dxer.github.io/2016/03/18/hbase/"/>
    <id>http://dxer.github.io/2016/03/18/hbase/</id>
    <published>2016-03-18T11:25:38.000Z</published>
    <updated>2016-05-18T09:39:14.833Z</updated>
    
    <content type="html">&lt;h2 id=&quot;HBase一些基本概念&quot;&gt;&lt;a href=&quot;#HBase一些基本概念&quot; class=&quot;headerlink&quot; title=&quot;HBase一些基本概念&quot;&gt;&lt;/a&gt;HBase一些基本概念&lt;/h2&gt;&lt;p&gt;1、Row key&lt;/p&gt;
&lt;p&gt;行主键，在对HBase进行查询时候只能依靠Row key，HBase不支持条件查询等类似于一些主流数据库的查询方式，读取记录只能依赖行主键以及进行全局扫面，可以将行主键想象成主流数据库查询过程中用到的主键（例如，id）。&lt;/p&gt;
&lt;p&gt;2、Column Family&lt;/p&gt;
&lt;p&gt;列族，可以将列族想象成日常主流数据库中的表结构的所有列的一个大管家，列族中存储了所有列的名称，整个表包括多少列，列族就包括多少（除去Row key和Timestamp列）。&lt;/p&gt;
&lt;p&gt;3、Column&lt;/p&gt;
&lt;p&gt;列，HBase的每个列都隶属于一个列族，以列族名称作为前缀，同一列族中的所有列会聚集在一个存储单元上，同时按照Column key进行排序。&lt;/p&gt;
&lt;p&gt;4、Timestamp&lt;/p&gt;
&lt;p&gt;在HBase中，通过row key 和 Colum Family确定一份数据，同一个row key和Colum Family可能有多份不同的数据，HBase通过时间戳来区分这些数据，同时按照时间戳对左右的数据进行排序，最新的数据排在最前面，时间戳默认为系统当前时间（精确到毫秒），同时也可以人为设置该值。&lt;/p&gt;
&lt;p&gt;5、Value&lt;/p&gt;
&lt;p&gt;我们在HBase表中精确查询数据时，通过TableName找到表，接着通过Row key找到对应的行，然后通过ColumnKey找到相应的列，最后根据时间戳找到最新的需要查询的值，这个值就是value。&lt;/p&gt;
&lt;p&gt;6、存储类型&lt;/p&gt;
&lt;p&gt;在HBase中，表名称是字符串，行键和列名称是二进制值（即就是Java中的Byte[]），时间戳是一个64为的整数（Java中的long类型），最后的查询结果Value是字节数组（Java中的byte[]类型）。&lt;/p&gt;
&lt;p&gt;7、存储结构&lt;/p&gt;
&lt;p&gt;在HBase中，整个数据表是按照行键进行排序，每行包括任意数量的列，列和列之间通过列键进行排序，每列包括若干的数据，整个HBase的存储结构可以理解如下：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Table(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	Row key，List(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		SortedMap(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			Column，list(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			Value，Timestamp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&quot;HBase数据模型&quot;&gt;&lt;a href=&quot;#HBase数据模型&quot; class=&quot;headerlink&quot; title=&quot;HBase数据模型&quot;&gt;&lt;/a&gt;HBase数据模型&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;表（table）：HBase用表组织数据。表名是字符串（String），由可以在文件系统路径里使用的字符组成。&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;行（row）：在表里，数据按行存储。行由行健（rowkey）唯一标识。行健没有数据类型，视为字节数组byte[]。&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;列族（column family）：行里的数据按照列族进行分组，列族也影响到HBase数据的物理存放。因此，他们必须事前定义并且不轻易修改。表中每行拥有相同列族，尽管行不需要在每个列族里存储数据，列族名字是字符串（String），由可以在文件系统路径里使用的字符组成。&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;列限定符（column qualifier）：列族里的数据通过列限定符或列来定位。列限定符不必事前定义。列限定符不必在不同行之间保持一致。就像行健一样，类限定符没有数据类型，视为字节数据byte[]。&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;单元（cell）：行健、列族和列限定符一起确定一个单元。存储在单元里的数据成为单元值（value）。值没有数据类型，视为字节数组byte[]。&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;时间版本（version）：单元值有时间版本。时间版本用时间戳标识，是一个long型。没有指定时间版本时，当前时间戳作为操作的基础。HBase保留单元值时间版本的数量基于列族进行配置。默认3个版本。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;数据坐标&lt;/p&gt;
&lt;p&gt;行健、列族、列限定符和时间版本&lt;/p&gt;
&lt;h5 id=&quot;写&quot;&gt;&lt;a href=&quot;#写&quot; class=&quot;headerlink&quot; title=&quot;写&quot;&gt;&lt;/a&gt;写&lt;/h5&gt;&lt;p&gt;执行写入的时候会写到两个文件：预写式日志（write-ahead log，也称HLog）和MemStore。HBase的默认方式是把写入动作记录在这两个地方，以保证数据持久化。只有当这两个地方的变化信息都写入并确认后，才认为写动作完成。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MemStore&lt;/em&gt;是内存里的写入缓冲区，HBase中数据在永久写入硬盘之前在这里积累。当MemStore填满后，其中的数据会刷写到硬盘，生成一个HFile文件。HFile是HBase使用底层存储格式。HFile对应列族，一个列族可以有多个HFile，但一个HFile不能存储多个列族的数据。在集群的每个节点上，每个列族都有一个MemStore。&lt;/p&gt;
&lt;p&gt;注：&lt;/p&gt;
&lt;p&gt;不写入WAL会在RegionServer故障时增加丢失数据的风险。关闭WAL，出现故障时HBase可能无法恢复数据，没有刷写到硬盘的所有写入数据都会丢失。&lt;/p&gt;
&lt;h4 id=&quot;读&quot;&gt;&lt;a href=&quot;#读&quot; class=&quot;headerlink&quot; title=&quot;读&quot;&gt;&lt;/a&gt;读&lt;/h4&gt;&lt;p&gt;HBase在读操作上使用了LRU（最近最少使用算法）缓存技术。也叫作BlockCache。BlockCache设计用来保存从HFile里读入内存的频繁访问的数据，避免硬盘读。每个列族都有自己的BlockCache。&lt;/p&gt;
&lt;p&gt;BlockCache中的Block是HBase从硬盘完成一次读取的数据单位。HFile物理存放形式是一个Block的序列外加这些Block的索引。从HBase中读取一个Block需要先将索引上查找一次改Block然后从硬盘读出、Block是建立索引的最小数据单位，也是从硬盘读取的最小数据单位。Block大小按照列族设定，默认是64kb。如果主要用于随机查询，可能需要细粒度的Block索引，小一点儿的Block更好一些。Block变小会导致索引变大，进而消耗更多内存。如果经常执行顺序扫描，一次读取多个Block，大一点的Block更好一些。Block变大意味着索引项变少，索引编写，因此节省内存。&lt;/p&gt;
&lt;p&gt;从HBase总读出一行，首先会检查MemStore等待修改的队列，然后检查BlockCache看包含改行的Blocj是否最近被访问过，最后访问硬盘上对应的HFile。&lt;/p&gt;
&lt;h4 id=&quot;删&quot;&gt;&lt;a href=&quot;#删&quot; class=&quot;headerlink&quot; title=&quot;删&quot;&gt;&lt;/a&gt;删&lt;/h4&gt;&lt;p&gt;执行HBase删除命令的时候，实际上数据并不会立即删除，只是会在该数据上打上删除的记录。被标记的记录不能在Get和Scan命令中返回结果。因为HFile文件是不能改变的，直到执行一次大合并，含有这些标记的数据才会被处理，被删除的数据占用的空间才会释放。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;大合并（major compaction）&lt;/em&gt;：处理给定region的一个列族的所有HFile。大合并完成后，这个列族的所有HFile合并成一个文件，可以从Shell中收工出发整个表（或者特定region）的大合并。大合并是HBase清理被删除记录的唯一机会&lt;/p&gt;
&lt;p&gt;&lt;em&gt;小合并（minor compaction）&lt;/em&gt;：把多个小HFile合并成一个大HFile。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;HBase-Shell命令&quot;&gt;&lt;a href=&quot;#HBase-Shell命令&quot; class=&quot;headerlink&quot; title=&quot;HBase Shell命令&quot;&gt;&lt;/a&gt;HBase Shell命令&lt;/h2&gt;&lt;p&gt;| Command | 描述 |&lt;/p&gt;
&lt;p&gt;|——–|——–|&lt;/p&gt;
&lt;p&gt;|alter        |修改列族模式       |&lt;/p&gt;
&lt;p&gt;|count        |统计表中行的数量        |&lt;/p&gt;
&lt;p&gt;|create        |创建表        |&lt;/p&gt;
&lt;p&gt;|describe   |显示表相关的详细信息        |&lt;/p&gt;
&lt;p&gt;|delete        |删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）        |&lt;/p&gt;
&lt;p&gt;|deleteall        |删除指定行的所有元素值        |&lt;/p&gt;
&lt;p&gt;|disable        |使表无效        |&lt;/p&gt;
&lt;p&gt;|enable        |使表有效        |&lt;/p&gt;
&lt;p&gt;|drop        |删除表        |&lt;/p&gt;
&lt;p&gt;|exists        |测试表是否存在        |&lt;/p&gt;
&lt;p&gt;|exit        |退出HBase Shell的值        |&lt;/p&gt;
&lt;p&gt;|get        |获取行或单元的值        |&lt;/p&gt;
&lt;p&gt;|incr        |增加指定表、行或列的值        |&lt;/p&gt;
&lt;p&gt;|list        |列出HBase中存在的所有表        |&lt;/p&gt;
&lt;p&gt;|put        |向指定的表单元添加值        |&lt;/p&gt;
&lt;p&gt;|tools        |列出HBase所支持的工具        |&lt;/p&gt;
&lt;p&gt;|scan        |通过对表的扫描来获取对应的值        |&lt;/p&gt;
&lt;p&gt;|status        | 返回HBase集群的状态信息       |&lt;/p&gt;
&lt;p&gt;|shutdown        |关闭HBase集群（与exit不同）        |&lt;/p&gt;
&lt;p&gt;|truncate        |重新创建指定表        |&lt;/p&gt;
&lt;p&gt;|version        |返回HBase版本信息        |&lt;/p&gt;
&lt;h4 id=&quot;create&quot;&gt;&lt;a href=&quot;#create&quot; class=&quot;headerlink&quot; title=&quot;create&quot;&gt;&lt;/a&gt;create&lt;/h4&gt;&lt;p&gt;创建表&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;create &amp;apos;t1&amp;apos;, &amp;#123;NAME =&amp;gt; &amp;apos;f1&amp;apos;, VERSIONS =&amp;gt; 5&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;create &amp;apos;t1&amp;apos;, &amp;#123;Name =&amp;gt; &amp;apos;f1&amp;apos;&amp;#125;, &amp;#123;NAME =&amp;gt; &amp;apos;f2&amp;apos;&amp;#125;, &amp;#123;NAME =&amp;gt; &amp;apos;f3&amp;apos;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#等价于：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;create &amp;apos;t1&amp;apos;, &amp;apos;cf&amp;apos;, &amp;apos;cf2&amp;apos;, &amp;apos;cf3&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;list&quot;&gt;&lt;a href=&quot;#list&quot; class=&quot;headerlink&quot; title=&quot;list&quot;&gt;&lt;/a&gt;list&lt;/h4&gt;&lt;p&gt;列出HBase中包含的表名称&lt;/p&gt;
&lt;h4 id=&quot;put&quot;&gt;&lt;a href=&quot;#put&quot; class=&quot;headerlink&quot; title=&quot;put&quot;&gt;&lt;/a&gt;put&lt;/h4&gt;&lt;p&gt;向指定表中添加值&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;put &amp;apos;t1&amp;apos;, &amp;apos;rowkey&amp;apos;, &amp;apos;cf:name&amp;apos;, &amp;apos;test&amp;apos;, ts&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#向t1表的rowkey，列cf:name添加值name，并指定时间戳为ts&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;####　scan&lt;/p&gt;
&lt;p&gt;对表的扫描来获取对应的值&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scan &amp;apos;t1&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scan &amp;apos;t1&amp;apos;, &amp;#123;COLUMNS =&amp;gt; &amp;apos;cf&amp;apos;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scan &amp;apos;t1&amp;apos;, &amp;#123;COLUMNS =&amp;gt; &amp;apos;cf&amp;apos;,  LIMIT =&amp;gt; 10&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;get&quot;&gt;&lt;a href=&quot;#get&quot; class=&quot;headerlink&quot; title=&quot;get&quot;&gt;&lt;/a&gt;get&lt;/h4&gt;&lt;p&gt;获取行或者单元的值。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;get &amp;apos;t1&amp;apos;, &amp;apos;rowkey&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;get &amp;apos;t1&amp;apos;, &amp;apos;rowkey&amp;apos;, &amp;#123;COLUMN =&amp;gt; &amp;apos;cf:name&amp;apos;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;注：COLUMN和COLUMNS是不同的，scan操作中的COLUMNS指定的是表的列族，get操作中的COLUMN指定的是特定的列，COLUMN的值是指上是“列族+列修饰符”&lt;/em&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;HBase一些基本概念&quot;&gt;&lt;a href=&quot;#HBase一些基本概念&quot; class=&quot;headerlink&quot; title=&quot;HBase一些基本概念&quot;&gt;&lt;/a&gt;HBase一些基本概念&lt;/h2&gt;&lt;p&gt;1、Row key&lt;/p&gt;
&lt;p&gt;行主键，在对HBase进行查询时候
    
    </summary>
    
    
      <category term="hbase" scheme="http://dxer.github.io/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hive入门</title>
    <link href="http://dxer.github.io/2016/03/11/hive02/"/>
    <id>http://dxer.github.io/2016/03/11/hive02/</id>
    <published>2016-03-11T02:14:20.000Z</published>
    <updated>2016-04-28T10:43:14.167Z</updated>
    
    <content type="html">&lt;p&gt;Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;那什么是数据仓库呢&quot;&gt;&lt;a href=&quot;#那什么是数据仓库呢&quot; class=&quot;headerlink&quot; title=&quot;那什么是数据仓库呢?&quot;&gt;&lt;/a&gt;那什么是数据仓库呢?&lt;/h2&gt;&lt;p&gt;数据仓库是一个面向主题的，集成的，不可更新的，随时间不变化的数据集合，它用于支持企业或组织的决策分析处理(主要是查询操作),数据仓库实际上就是一个数据库，可以利用数据仓库来保存数据，但是数据仓库有别于我们一般的数据库&lt;/p&gt;
&lt;h2 id=&quot;数据仓库的结构和建立过程&quot;&gt;&lt;a href=&quot;#数据仓库的结构和建立过程&quot; class=&quot;headerlink&quot; title=&quot;数据仓库的结构和建立过程&quot;&gt;&lt;/a&gt;数据仓库的结构和建立过程&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;数据源（业务数据系统，文档资料，其他数据）&lt;br&gt;数据存储和管理（ETL，抽取Extract，转换Transform，装载Load）&lt;br&gt;数据仓库引擎&lt;br&gt;前端展示（数据查询，数据报表，数据分析，各类应用）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;什么是Hive？&quot;&gt;&lt;a href=&quot;#什么是Hive？&quot; class=&quot;headerlink&quot; title=&quot;什么是Hive？&quot;&gt;&lt;/a&gt;什么是Hive？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;hive是建立在hadoop hdfs上的数据仓库基础架构,hive中的表和文件其实就是hdfs中的目录和文件&lt;/li&gt;
&lt;li&gt;hive可以用来进行数据提取转化加载（ETL）&lt;/li&gt;
&lt;li&gt;hive定义了简单的类似SQL查询语言（HQL），它允许熟悉SQL的用户查询数据&lt;/li&gt;
&lt;li&gt;hive允许熟悉MapReduce开发者开发自定义的mapper和reducer来处理內建的mapper和reducer无法完成的复杂的分析工作&lt;/li&gt;
&lt;li&gt;hive是SQL解析引擎，它将SQL语句转义成MapReduce Job然后在hadoop上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;hive的元数据&quot;&gt;&lt;a href=&quot;#hive的元数据&quot; class=&quot;headerlink&quot; title=&quot;hive的元数据&quot;&gt;&lt;/a&gt;hive的元数据&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;hive将元数据存储在数据库中（metastore），支持mysql和derby（默认）等数据库&lt;br&gt;hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;HQL的执行过程&quot;&gt;&lt;a href=&quot;#HQL的执行过程&quot; class=&quot;headerlink&quot; title=&quot;HQL的执行过程&quot;&gt;&lt;/a&gt;HQL的执行过程&lt;/h2&gt;&lt;p&gt;解释器，编译器，优化器完成HQL查询语句从此法分析，语法分析，编译，优化以及查询计划（plan）的生成。生成的查询计划存储在HDFS中，并在随后又MapReduce调用执行&lt;/p&gt;
&lt;p&gt;HQL执行过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HQL select语句&lt;/li&gt;
&lt;li&gt;解释器进行此法分析&lt;/li&gt;
&lt;li&gt;编译器生成HQL的执行计划（类似javac命令，将java文件编译成class文件）&lt;/li&gt;
&lt;li&gt;优化器生成最佳的执行计划&lt;/li&gt;
&lt;li&gt;执行&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;hive的三种安装模式&quot;&gt;&lt;a href=&quot;#hive的三种安装模式&quot; class=&quot;headerlink&quot; title=&quot;hive的三种安装模式&quot;&gt;&lt;/a&gt;hive的三种安装模式&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;嵌入模式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;元数据信息保存在hive自带的derby数据库中&lt;/li&gt;
&lt;li&gt;只允许创建一个连接&lt;/li&gt;
&lt;li&gt;多用于Demo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;本地模式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;元数据信息被存储在MySQL数据库中&lt;/li&gt;
&lt;li&gt;MySQL数据库与Hive运行在同一台物理机器上&lt;/li&gt;
&lt;li&gt;多用于开发和测试&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;远程模式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;元数据信息被保存在远程的MySQL数据库中&lt;/li&gt;
&lt;li&gt;多用于实际的生产运行环境&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;常用的CLI命令&quot;&gt;&lt;a href=&quot;#常用的CLI命令&quot; class=&quot;headerlink&quot; title=&quot;常用的CLI命令&quot;&gt;&lt;/a&gt;常用的CLI命令&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;进入CLI&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;交互模式&lt;br&gt;hive  # 进入命令行交互模式，非静默&lt;br&gt;hive -S # 静默模式，不显示调试信息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;直接执行一条语句&lt;br&gt;hive -e ‘show tables’;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;执行一个文件的文件&lt;br&gt;hive -f ~/test.hql&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;清屏&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ctrl + L或者 !clear;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查看数据仓库中的表&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;show tables;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查看数据仓库中内置的函数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;show functions;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查看表的结构&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;desc 表名&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查看HDFS上的文件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dfs -ls 目录&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;执行操作系统的命令&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;!命令&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;执行HQL语句&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;select &lt;strong&gt;&lt;em&gt; from &lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;执行SQL的脚本&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;source SQL文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;hive的数据类型&quot;&gt;&lt;a href=&quot;#hive的数据类型&quot; class=&quot;headerlink&quot; title=&quot;hive的数据类型&quot;&gt;&lt;/a&gt;hive的数据类型&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基本数据类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tinyint/smallint/int/bigint：整数类型&lt;/li&gt;
&lt;li&gt;float/double：浮点数类型&lt;/li&gt;
&lt;li&gt;boolean：布尔类型&lt;/li&gt;
&lt;li&gt;string：字符串类型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;复杂数据类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Array：数组类型，由一系列相同数据类型的元素组成&lt;/li&gt;
&lt;li&gt;Map：集合类型，包含key-&amp;gt;value，可以通过key来访问元素&lt;/li&gt;
&lt;li&gt;Struct：结构类型，可以包含不同数据类型的元素，这些元素可以通过“点语法”的方式来得到所需要的元素&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;时间类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date：从Hive0.12.0开始支持&lt;/li&gt;
&lt;li&gt;Timestamp：从Hive0.8.0开始支持&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;TABLE&lt;/span&gt; employees(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;salary &lt;span class=&quot;built_in&quot;&gt;FLOAT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;subordintes &lt;span class=&quot;built_in&quot;&gt;ARRAY&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;&amp;gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;deucations &lt;span class=&quot;keyword&quot;&gt;MAP&lt;/span&gt;&amp;lt;&lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;, &lt;span class=&quot;built_in&quot;&gt;FLOAT&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;address &lt;span class=&quot;keyword&quot;&gt;STRUCT&lt;/span&gt;&amp;lt;street:&lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;, city:&lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;, state:&lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;, zip:&lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Hive中的数据都是保存在HDFS中&lt;br&gt;没有专门的数据存储格式&lt;br&gt;存储结构主要包括：数据库，文件，表，视图&lt;br&gt;Hive可以直接加载文本文件&lt;br&gt;创建表的时候，指定Hive数据的列分隔符与行分隔符&lt;/p&gt;
&lt;h6 id=&quot;数据库database&quot;&gt;&lt;a href=&quot;#数据库database&quot; class=&quot;headerlink&quot; title=&quot;数据库database&quot;&gt;&lt;/a&gt;数据库database&lt;/h6&gt;&lt;ul&gt;
&lt;li&gt;相当于关系数据库中的命名空间（namespace），它的作用是将用户和数据库的应用隔离到不同的数据库或模式中&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;表&quot;&gt;&lt;a href=&quot;#表&quot; class=&quot;headerlink&quot; title=&quot;表&quot;&gt;&lt;/a&gt;表&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Table内部表&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;与数据库中的Table在概念上是类似的&lt;/li&gt;
&lt;li&gt;每一个Table在Hive中都有一个相应的目录存储数据&lt;/li&gt;
&lt;li&gt;所有的Table数据（不包括External Table）都保存在这个目录中&lt;/li&gt;
&lt;li&gt;删除表的时候，元数据与数据都会被删除&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Partition分区表&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partition对应于数据库的Partition列的密集索引&lt;/li&gt;
&lt;li&gt;在Hive中，表中的一个Partition对应于表下的一个目录，多有的Partition的数据都存储在对应的目录中&lt;/li&gt;
&lt;li&gt;在数据量特别大的时候，需要将数据按照一定的条件进行分区，这样在进行查询操作的时候能够降低扫描的数据，从而提高查询的效率（通过执行计划知道）&lt;/li&gt;
&lt;li&gt;Hive把表组织成”分区“，这是一种根据“分区列”（如日期）的值对表的数据进行粗略划分的机制。使用分区可以加快数据分片（slice）的查询速度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;External Table 外部表&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指向已经存在于HDFS中的数据，也可以创建Patition&lt;/li&gt;
&lt;li&gt;它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异&lt;/li&gt;
&lt;li&gt;外部表只有一个过程，加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个外部表是，仅删除该链接&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bucket Table 桶表&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;桶表是对数据进行哈希取值，然后放到不同文件中存储&lt;/li&gt;
&lt;li&gt;降低系统的热块，从而提高查询的速度&lt;/li&gt;
&lt;li&gt;表和分区可以进一步分为“桶”，它会为数据提供额外的结构以获得更高效的查询处理。例如，可以根据用户ID来划分桶，这则是对数据源数据文件本身来拆分数据。使用桶的表会将源数据文件按一定规律拆分成多个文件，要使用bucket&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;视图&quot;&gt;&lt;a href=&quot;#视图&quot; class=&quot;headerlink&quot; title=&quot;视图&quot;&gt;&lt;/a&gt;视图&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;视图是一种虚表，是一个逻辑概念，不存数据；可以跨越多张表&lt;/li&gt;
&lt;li&gt;视图建立在已有表的基础上，视图赖以建立的这些表成为基表&lt;/li&gt;
&lt;li&gt;视图可以简化复杂的查询&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;内部表和外部表的区别？&quot;&gt;&lt;a href=&quot;#内部表和外部表的区别？&quot; class=&quot;headerlink&quot; title=&quot;内部表和外部表的区别？&quot;&gt;&lt;/a&gt;内部表和外部表的区别？&lt;/h2&gt;&lt;p&gt;内部表也叫做管理表，Hive会控制着数据的生命周期，默认情况下会将这些表的数据存储在由配置项&lt;code&gt;hive.metastore.warehourse.dir&lt;/code&gt;所定义的目录的子目录下&lt;br&gt;当删除一个内部表的时候，Hive也会删除这个表中的数据&lt;/p&gt;
&lt;p&gt;外部表&lt;br&gt;先看一个例子：&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;external&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;exists&lt;/span&gt; pimaccess(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    accesstime &lt;span class=&quot;built_in&quot;&gt;BIGINT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;account&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ip &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    appkey &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    prelinkmannum &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    aftlinkmannum &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    pregroupnum &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    aftgroupnum &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    resultcode &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    costtime &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    requesttype &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    responsetype &lt;span class=&quot;built_in&quot;&gt;INT&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    apiname &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    requestdata &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    responsedata &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    rtime &lt;span class=&quot;keyword&quot;&gt;STRING&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;)&lt;span class=&quot;keyword&quot;&gt;ROW&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;FORMAT&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;DELIMITED&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;FIELDS&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;TERMINATED&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;\t&#39;&lt;/span&gt; LOCATION &lt;span class=&quot;string&quot;&gt;&#39;/pimaccess&#39;&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在创建表的时候使用&lt;code&gt;EXTENAL&lt;/code&gt;关键字，用来告诉Hive这个表是外部表，&lt;code&gt;LOCATION&lt;/code&gt;子句则用于告诉Hive数据位于哪个路径下。&lt;br&gt;对于外部表，Hive认为没有完全拥有这份数据，因此在删除该表的时候不会删除掉这份数据，不过描述表的元数据信息会被删除。&lt;/p&gt;
&lt;h2 id=&quot;Hive常见的数据导入方式&quot;&gt;&lt;a href=&quot;#Hive常见的数据导入方式&quot; class=&quot;headerlink&quot; title=&quot;Hive常见的数据导入方式&quot;&gt;&lt;/a&gt;Hive常见的数据导入方式&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;从本地文件系统中导入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;load data local inpath ‘test.txt’ into table test;&lt;/li&gt;
&lt;li&gt;load data local inpath ‘test.txt’ overwrite into table test;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从HDFS上导入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;load data inpath ‘/test.txt’ into table test;&lt;/li&gt;
&lt;li&gt;load data inpath ‘/test.txt’ overwrite into table test;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从别的表中查询出相应数据并导入 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;insert into table test partition(age=’25’) select id, name, tel from test;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create table test2 as select id, name, tel from test;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。&lt;br&gt;
    
    </summary>
    
    
      <category term="bigdata" scheme="http://dxer.github.io/tags/bigdata/"/>
    
      <category term="hive" scheme="http://dxer.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>hive的部署安装</title>
    <link href="http://dxer.github.io/2016/03/10/hive01/"/>
    <id>http://dxer.github.io/2016/03/10/hive01/</id>
    <published>2016-03-10T06:29:19.000Z</published>
    <updated>2016-04-25T00:42:30.581Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="bigdata" scheme="http://dxer.github.io/tags/bigdata/"/>
    
      <category term="hive" scheme="http://dxer.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper伪集群模式安装和配置</title>
    <link href="http://dxer.github.io/2016/03/07/zookeeper01/"/>
    <id>http://dxer.github.io/2016/03/07/zookeeper01/</id>
    <published>2016-03-07T02:25:38.000Z</published>
    <updated>2016-04-28T10:39:34.349Z</updated>
    
    <content type="html">&lt;p&gt;伪分布式集群：在一台Server中，启动多个ZooKeeper的实例。&lt;br&gt;下面来看看zookeeper怎么进行安装的吧。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerlink&quot; title=&quot;下载&quot;&gt;&lt;/a&gt;下载&lt;/h2&gt;&lt;p&gt;选择一个稳定版本进行下载，我这里下载的是zookeeper-3.4.6版本。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;解压&quot;&gt;&lt;a href=&quot;#解压&quot; class=&quot;headerlink&quot; title=&quot;解压&quot;&gt;&lt;/a&gt;解压&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tar -zxvf  zookeeper-3.4.6.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 3个实例，复制三份 zk1，zk2，zk3&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp -r zookeeper-3.4.6 zk1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp -r zookeeper-3.4.6 zk2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp -r zookeeper-3.4.6 zk3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;创建实例配置文件&quot;&gt;&lt;a href=&quot;#创建实例配置文件&quot; class=&quot;headerlink&quot; title=&quot;创建实例配置文件&quot;&gt;&lt;/a&gt;创建实例配置文件&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd zookeeper-3.4.6/conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp zoo_sample.cfg zoo1.cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp zoo_sample.cfg zoo2.cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cp zoo_sample.cfg zoo3.cfg&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;修改配置&quot;&gt;&lt;a href=&quot;#修改配置&quot; class=&quot;headerlink&quot; title=&quot;修改配置&quot;&gt;&lt;/a&gt;修改配置&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tickTime=2000  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;dataDir=/opt/zk1/data  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;clientPort=2181  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;initLimit=10  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;syncLimit=5  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;server.1=127.0.0.1:2881:3881  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;server.2=127.0.0.1:2882:3882  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;server.3=127.0.0.1:2883:3883&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;分别配置三个实例的clientPort端口为2181, 2182, 2183&lt;/li&gt;
&lt;li&gt;分别配置是哪个实例的dataDir目录为&lt;code&gt;/opt/zk1/data&lt;/code&gt;，&lt;code&gt;/opt/zk2/data&lt;/code&gt;，&lt;code&gt;/opt/zk3/data&lt;/code&gt;，并创建这三个目录,没有创建该目录会启动出错&lt;/li&gt;
&lt;li&gt;定义zookeeper集群的各个实例的ip和端口，server.1=127.0.0.1:2881:3881 ,server.2=127.0.0.1:2882:3882,server.3=127.0.0.1:2883:3883 &lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;dataDir&lt;br&gt;定义zookeeper实例存储持久出具的本地文件系统位置&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clientPort&lt;br&gt;定义zookeeper客户端连接zookeeper服务端时使用的端口&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;server&lt;br&gt;定义zookeeper集群的各个实例的ip和端口&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;tickTime&lt;br&gt;指定了zookeeper中的基本时间单元（以毫秒为单位）&lt;br&gt;zookeeper集群中，每个服务器都有一个id（数字），服务器id在集群中是唯一的，并且取值范围是1~255，通过一个名为myid的纯文本设置，这个文件保存在dataDir中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;server.n=hostname:port:port&lt;br&gt;n是服务器id，第一个port是follower用来连接leader的端口，第二个port是用于leader选举&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clientPort&lt;br&gt;监听client连接的端口号&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;initLimit&lt;br&gt;设定了所有follower与leader进行连接并同步的时间范围。如果在设定的时间段内，半数以上的follower跟随者未能完成同步，leader会宣布放弃领导地位，然后进行另外一次leader选举，如果这种情况经常发生，则表明设定的值太小&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;syncLimit&lt;br&gt;设定了允许一个follower与leader这进行同步的时间。如果在设定的时间段内，一个follower未能完成同步，会自己重启，所有关联到follower的客户端将连接到另一个follower&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h6 id=&quot;创建dataDir和实例id文件&quot;&gt;&lt;a href=&quot;#创建dataDir和实例id文件&quot; class=&quot;headerlink&quot; title=&quot;创建dataDir和实例id文件&quot;&gt;&lt;/a&gt;创建dataDir和实例id文件&lt;/h6&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;mkdir -p /opt/zk1/data  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mkdir -p /opt/zk2/data  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mkdir -p /opt/zk3/data  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; 1 &amp;gt; /opt/zk1/data/myid  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; 2 &amp;gt; /opt/zk2/data/myid  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; 3 &amp;gt; /opt/zk3/data/myid&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;启动zookeeper服务&quot;&gt;&lt;a href=&quot;#启动zookeeper服务&quot; class=&quot;headerlink&quot; title=&quot;启动zookeeper服务&quot;&gt;&lt;/a&gt;启动zookeeper服务&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;bin/zkServer.sh start zoo1.cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;bin/zkServer.sh start zoo2.cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;bin/zkServer.sh start zoo3.cfg&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;查看是否启动成功&quot;&gt;&lt;a href=&quot;#查看是否启动成功&quot; class=&quot;headerlink&quot; title=&quot;查看是否启动成功&quot;&gt;&lt;/a&gt;查看是否启动成功&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;jps&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;看到类似下面的进程就表示3个实例均启动成功&lt;br&gt;13419 QuorumPeerMain&lt;br&gt;13460 QuorumPeerMain&lt;br&gt;13561 Jps&lt;br&gt;13392 QuorumPeerMain&lt;br&gt;如果未成功启动，可以到zookeeper.out文件中查看启动失败的日志信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;客户端连接&quot;&gt;&lt;a href=&quot;#客户端连接&quot; class=&quot;headerlink&quot; title=&quot;客户端连接&quot;&gt;&lt;/a&gt;客户端连接&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./zkCli.sh -server 127.0.0.1:2181&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;关闭zookeeper&quot;&gt;&lt;a href=&quot;#关闭zookeeper&quot; class=&quot;headerlink&quot; title=&quot;关闭zookeeper&quot;&gt;&lt;/a&gt;关闭zookeeper&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;bin/zkServer.sh stop&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;重启zookeeper&quot;&gt;&lt;a href=&quot;#重启zookeeper&quot; class=&quot;headerlink&quot; title=&quot;重启zookeeper&quot;&gt;&lt;/a&gt;重启zookeeper&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;bin/zkServer.sh restart&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;伪分布式集群：在一台Server中，启动多个ZooKeeper的实例。&lt;br&gt;下面来看看zookeeper怎么进行安装的吧。&lt;br&gt;
    
    </summary>
    
    
      <category term="zookeeper" scheme="http://dxer.github.io/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Neo4j - Cypher</title>
    <link href="http://dxer.github.io/2015/12/02/neo4j-cypher/"/>
    <id>http://dxer.github.io/2015/12/02/neo4j-cypher/</id>
    <published>2015-12-02T04:17:19.000Z</published>
    <updated>2016-05-13T01:13:39.707Z</updated>
    
    <content type="html">&lt;p&gt;Neo4j是一种图数据库。它将结构化数据存储在图上而不是传统的数据库表中。 相对于关系数据库来说，图数据库善于处理大量复杂、互连接、低结构化的数据，这些数据变化迅速，需要频繁的查询。在关系数据库中， 这些查询会导致大量的表连接，因此会产生性能上的问题,但是使用Neo4j就可以解决查询时出现的性能衰退问题。 同时Neo4j还提供了非常快的图算法、推荐系统和OLAP风格的分析。下面主要简单记录一下Cypher查询语言的使用方法。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个查询语言包含以下几个明显的部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;START：在图中的开始点，通过元素的ID或所以查找获得。&lt;/li&gt;
&lt;li&gt;MATCH：图形的匹配模式，束缚于开始点。&lt;/li&gt;
&lt;li&gt;WHERE：过滤条件。&lt;/li&gt;
&lt;li&gt;RETURN：返回所需要的。&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Operators&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mathematical&lt;/td&gt;
&lt;td&gt;+, -, *, /, %, ^&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Comparison&lt;/td&gt;
&lt;td&gt;=, &amp;lt;&amp;gt;, &amp;lt;, &amp;gt;, &amp;lt;=, &amp;gt;=&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Boolean&lt;/td&gt;
&lt;td&gt;AND, OR, XOR, NOT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;String&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Collection&lt;/td&gt;
&lt;td&gt;+, IN, [x], [x .. y]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Regular Expression&lt;/td&gt;
&lt;td&gt;=~&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;String matching&lt;/td&gt;
&lt;td&gt;STARTS WITH, ENDS WITH, CONTAINS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&quot;Create&quot;&gt;&lt;a href=&quot;#Create&quot; class=&quot;headerlink&quot; title=&quot;Create&quot;&gt;&lt;/a&gt;Create&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;创建节点&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- 创建单节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (n)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- 创建多节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (n),(m)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- 创建一个节点并含有一个label&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (n:Person) &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- 创建一个节点并含有多个label&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (n:Person:Swedish) &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- 创建一个节点并含有label和properties&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (n:Person &amp;#123; &lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt; : &lt;span class=&quot;string&quot;&gt;&#39;Andres&#39;&lt;/span&gt;, title : &lt;span class=&quot;string&quot;&gt;&#39;Developer&#39;&lt;/span&gt; &amp;#125;) &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;-- 创建一个节点并返回该节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (a &amp;#123; &lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt; : &lt;span class=&quot;string&quot;&gt;&#39;Andres&#39;&lt;/span&gt; &amp;#125;) &lt;span class=&quot;keyword&quot;&gt;RETURN&lt;/span&gt; a&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;创建关系&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MATCH (a:Person),(b:Person)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;WHERE a.name = &#39;Node A&#39; AND b.name = &#39;Node B&#39;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (a)-[r:RELTYPE]-&amp;gt;(b)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;RETURN&lt;/span&gt; r&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;创建关系并设置属性&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MATCH (a:Person),(b:Person)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;WHERE a.name = &#39;Node A&#39; AND b.name = &#39;Node B&#39;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; (a)-[r:RELTYPE &amp;#123; &lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt; : a.name + &lt;span class=&quot;string&quot;&gt;&#39;&amp;lt;-&amp;gt;&#39;&lt;/span&gt; + b.name &amp;#125;]-&amp;gt;(b)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;RETURN&lt;/span&gt; r&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;Merge&quot;&gt;&lt;a href=&quot;#Merge&quot; class=&quot;headerlink&quot; title=&quot;Merge&quot;&gt;&lt;/a&gt;Merge&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Use ON CREATE and ON MATCH&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;MERGE&lt;/span&gt; (keanu:Person &amp;#123; &lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt;:&lt;span class=&quot;string&quot;&gt;&#39;Keanu Reeves&#39;&lt;/span&gt; &amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;SET&lt;/span&gt; keanu.created = &lt;span class=&quot;keyword&quot;&gt;timestamp&lt;/span&gt;()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;RETURN&lt;/span&gt; keanu.name, keanu.created&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Merge with ON CREATE and ON MATCH&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;MERGE&lt;/span&gt; (keanu:Person &amp;#123; &lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt;:&lt;span class=&quot;string&quot;&gt;&#39;Keanu Reeves&#39;&lt;/span&gt; &amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;SET&lt;/span&gt; keanu.created = &lt;span class=&quot;keyword&quot;&gt;timestamp&lt;/span&gt;()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;MATCH&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;SET&lt;/span&gt; keanu.lastSeen = &lt;span class=&quot;keyword&quot;&gt;timestamp&lt;/span&gt;()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;RETURN&lt;/span&gt; keanu.name, keanu.created, keanu.lastSeen&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Merge relationships&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MATCH (charlie:Person &amp;#123; name:&#39;Charlie Sheen&#39; &amp;#125;),(wallStreet:Movie &amp;#123; title:&#39;Wall Street&#39; &amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MERGE (charlie)-[r:ACTED_IN]-&amp;gt;(wallStreet)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RETURN charlie.name, type(r), wallStreet.title&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MATCH (oliver:Person &amp;#123; name:&#39;Oliver Stone&#39; &amp;#125;),(reiner:Person &amp;#123; name:&#39;Rob Reiner&#39; &amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MERGE (oliver)-[:DIRECTED]-&amp;gt;(movie:Movie)&amp;lt;-[:ACTED_IN]-(reiner)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RETURN movie&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;查询&quot;&gt;&lt;a href=&quot;#查询&quot; class=&quot;headerlink&quot; title=&quot;查询&quot;&gt;&lt;/a&gt;查询&lt;/h3&gt;&lt;p&gt;语法&lt;br&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[MATCH WHERE] &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[OPTIONAL MATCH WHERE] &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[WITH [ORDER BY] [SKIP] [LIMIT]] &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RETURN [ORDER BY] [SKIP] [LIMIT]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;示例&lt;br&gt;&lt;figure class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MATCH (n:Person)-[:KNOWS]-&amp;gt;(m:Person)WHERE n.name=&quot;Alice&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Node patterns can contain labels and properties.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MATCH (n)--&amp;gt;(m)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Any pattern can be used in MATCH.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MATCH (n &amp;#123;name:&#39;Alice&#39;&amp;#125;)--&amp;gt;(m)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Patterns with node properties.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;MATCH p = (n)--&amp;gt;(m)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Assign a path to p.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;OPTIONAL MATCH (n)-[r]-&amp;gt;(m)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Optional pattern, NULLs will be used for missing parts&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&quot;唯一约束&quot;&gt;&lt;a href=&quot;#唯一约束&quot; class=&quot;headerlink&quot; title=&quot;唯一约束&quot;&gt;&lt;/a&gt;唯一约束&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;CREATE CONSTRAINT ON (p:Person) ASSERT p.name IS UNIQUE&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;DROP CONSTRAINT ON (p:Person) ASSERT p.name IS UNIQUE&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;索引&quot;&gt;&lt;a href=&quot;#索引&quot; class=&quot;headerlink&quot; title=&quot;索引&quot;&gt;&lt;/a&gt;索引&lt;/h3&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1. 创建索引&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;INDEX&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; :Person(&lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;2.&lt;/span&gt; 删除索引&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;DROP&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;INDEX&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;ON&lt;/span&gt; :Person(&lt;span class=&quot;keyword&quot;&gt;name&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;参考&quot;&gt;&lt;a href=&quot;#参考&quot; class=&quot;headerlink&quot; title=&quot;参考:&quot;&gt;&lt;/a&gt;参考:&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://neo4j.com/docs/stable/cypher-query-lang.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;neo4j官网Cypher文档&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Neo4j是一种图数据库。它将结构化数据存储在图上而不是传统的数据库表中。 相对于关系数据库来说，图数据库善于处理大量复杂、互连接、低结构化的数据，这些数据变化迅速，需要频繁的查询。在关系数据库中， 这些查询会导致大量的表连接，因此会产生性能上的问题,但是使用Neo4j就可以解决查询时出现的性能衰退问题。 同时Neo4j还提供了非常快的图算法、推荐系统和OLAP风格的分析。下面主要简单记录一下Cypher查询语言的使用方法。&lt;br&gt;
    
    </summary>
    
    
      <category term="Neo4j" scheme="http://dxer.github.io/tags/Neo4j/"/>
    
      <category term="Cypher" scheme="http://dxer.github.io/tags/Cypher/"/>
    
  </entry>
  
  <entry>
    <title>Neo4j</title>
    <link href="http://dxer.github.io/2015/12/02/neo4j-01/"/>
    <id>http://dxer.github.io/2015/12/02/neo4j-01/</id>
    <published>2015-12-02T01:13:19.000Z</published>
    <updated>2016-04-29T07:35:08.003Z</updated>
    
    <content type="html">&lt;p&gt;最近在用图形数据库来完成对项目的支持。在使用过程中觉得这种图形数据库实际上挺有意思的。因此在这里给大家做一个简单的介绍。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;图数据库&quot;&gt;&lt;a href=&quot;#图数据库&quot; class=&quot;headerlink&quot; title=&quot;图数据库&quot;&gt;&lt;/a&gt;图数据库&lt;/h2&gt;&lt;p&gt;图形数据库是一种非关系型数据库，它应用图形理论存储实体之间的关系信息。最常见的一个例子，就是社会网络中人与人之间的关系。关系型数据库用于存储“关系型”数据的效果并不好，其查询复杂、缓慢、超出预期，而图形数据库的独特设计恰恰弥补了这个缺陷。&lt;/p&gt;
&lt;h2 id=&quot;Neo4j&quot;&gt;&lt;a href=&quot;#Neo4j&quot; class=&quot;headerlink&quot; title=&quot;Neo4j&quot;&gt;&lt;/a&gt;Neo4j&lt;/h2&gt;&lt;p&gt;Neo4j是一个流行的图形数据库，它是开源的。Neo4j基于Java实现，兼容ACID特性，也支持其他编程语言，如Ruby和Python。&lt;/p&gt;
&lt;p&gt;Neo4j是一个高性能的,NOSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j 使用图（graph）相关的概念来描述数据模型，把数据保存为图中的节点以及节点之间的关系。很多应用中数据之间的关系，可以很直接地使用图中节点和关系的概念来建模。对于这样的应用，使用 Neo4j 来存储数据会非常的自然，要优于使用关系数据库。&lt;/p&gt;
&lt;p&gt;Neo4j 使用数据结构中图（graph）的概念来进行建模。Neo4j 中两个最基本的概念是节点和边。节点表示实体，边则表示实体之间的关系。节点和边都可以有自己的属性。不同实体通过各种不同的关系关联起来，形成复杂的对象图。Neo4j 同时提供了在对象图上进行查找和遍历的功能。&lt;/p&gt;
&lt;h3 id=&quot;Neo4j特点&quot;&gt;&lt;a href=&quot;#Neo4j特点&quot; class=&quot;headerlink&quot; title=&quot;Neo4j特点&quot;&gt;&lt;/a&gt;Neo4j特点&lt;/h3&gt;&lt;p&gt;作为一款强健的，可伸缩的高性能数据库，Neo4j最适合完整的企业部署或者用于一个轻量级项目中完整服务器的一个子集存在。&lt;/p&gt;
&lt;p&gt;它包括如下几个显著特点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完整的ACID支持&lt;/li&gt;
&lt;li&gt;高可用性&lt;/li&gt;
&lt;li&gt;轻易扩展到上亿级别的节点和关系&lt;/li&gt;
&lt;li&gt;通过遍历工具高速检索数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;适当的ACID操作是保证数据一致性的基础。Neo4j确保了在一个事务里面的多个操作同时发生，保证数据一致性。不管是采用嵌入模式还是多服务器集群部署，都支持这一特性。&lt;/p&gt;
&lt;p&gt;可靠的图型存储可以非常轻松的集成到任何一个应用中。随着我们开发的应用在运营中不断发展，性能问题肯定会逐步凸显出来，而Neo4j不管应用如何变化，他只会受到计算机硬件性能的影响，不受业务本身的约束。&lt;/p&gt;
&lt;p&gt;部署一个neo4j服务器便可以承载上亿级的节点和关系。当然，当单节点无法承载我们的数据需求时，我们可以进行分布式集群部署（含有集群方案的版本是商业版）。&lt;/p&gt;
&lt;p&gt;将图数据库用于存储关系复杂的数据是他最大的优势。通过Neo4j提供的遍历工具，可以非常高效的进行数据检索，每秒可以达到上亿级的检索量。一个检索操作类似于RDBMS里面的连接（&lt;em&gt;join&lt;/em&gt;）操作。&lt;/p&gt;
&lt;h3 id=&quot;Cypher查询语言&quot;&gt;&lt;a href=&quot;#Cypher查询语言&quot; class=&quot;headerlink&quot; title=&quot;Cypher查询语言&quot;&gt;&lt;/a&gt;Cypher查询语言&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Cypher&lt;/em&gt;查询语言，是一种不需要手动遍历图结构数据，就可高效完成查询功能的陈述性查询语言。Cypher用于在图数据库中存储和检索数据，可以实现对Neo4j数据库的添加、删除及更新操作。Cypher受启于陈述性查询语言，很多关键字如WHERE、ORDER BY等来源于SQL；模式匹配方法来源于SPARQL。Cypher语言更专注于检索内容的图结构形式化描述，而不是实现。&lt;/p&gt;
&lt;p&gt;在查询语言中包含以下几个明显的部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;START：在图中的开始点，通过元素的ID或所以查找获得。&lt;/li&gt;
&lt;li&gt;MATCH：图形的匹配模式，束缚于开始点。&lt;/li&gt;
&lt;li&gt;WHERE：过滤条件。&lt;/li&gt;
&lt;li&gt;RETURN：返回所需要的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;使用Neo4j图数据库的优势&quot;&gt;&lt;a href=&quot;#使用Neo4j图数据库的优势&quot; class=&quot;headerlink&quot; title=&quot;使用Neo4j图数据库的优势&quot;&gt;&lt;/a&gt;使用Neo4j图数据库的优势&lt;/h3&gt;&lt;p&gt;使用Neo4j图数据库的优势如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;自带一套易于学习的Cypher查询语言，减少在开发过程中的学习成本；&lt;/li&gt;
&lt;li&gt;与关系型数据库相比，对于高度关联的数据（图形数据）的查询快速要快上许多；&lt;/li&gt;
&lt;li&gt;它的实体与关系结构非常自然地切合人类的直观感受；&lt;/li&gt;
&lt;li&gt;支持兼容ACID的事务操作；&lt;/li&gt;
&lt;li&gt;提供了一个高可用性模型，以支持大规模数据量的查询，支持备份、数据局部性以及冗余；&lt;/li&gt;
&lt;li&gt;提供了一个可视化的查询控制台，方便在控制台进行查询操作。&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;最近在用图形数据库来完成对项目的支持。在使用过程中觉得这种图形数据库实际上挺有意思的。因此在这里给大家做一个简单的介绍。&lt;br&gt;
    
    </summary>
    
    
      <category term="Neo4j" scheme="http://dxer.github.io/tags/Neo4j/"/>
    
      <category term="NoSQL" scheme="http://dxer.github.io/tags/NoSQL/"/>
    
  </entry>
  
  <entry>
    <title>YARN学习</title>
    <link href="http://dxer.github.io/2015/11/03/yarn/"/>
    <id>http://dxer.github.io/2015/11/03/yarn/</id>
    <published>2015-11-03T06:46:19.000Z</published>
    <updated>2016-05-16T07:46:37.010Z</updated>
    
    <content type="html">&lt;p&gt;YARN采用Master/Slave架构，ResourceManager为Master，NodeManager为Slave。ResourceManager负责对各个NodeManager上的资源进行统一管理和调度，当用户提交一个应用程序时，需要提供一个用来跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManager启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。&lt;/p&gt;
&lt;h2 id=&quot;ResourceManager&quot;&gt;&lt;a href=&quot;#ResourceManager&quot; class=&quot;headerlink&quot; title=&quot;ResourceManager&quot;&gt;&lt;/a&gt;ResourceManager&lt;/h2&gt;&lt;p&gt;ResourceManager简称RM，是一个全局的资源管理器，负责整个系统的资源管理和分配工作。主要由调度器（Scheduler）和应用程序管理器（Applications Master，ASM）两部分组成。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调度器（Scheduler）&lt;br&gt;调度器根据容量，队列等限制条件将系统中的资源分配给各个正在运行的应用程序。&lt;/li&gt;
&lt;li&gt;应用程序管理器（Applications Master）&lt;br&gt;ApplicationMaster负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster，监控ApplicationMaster运行状态并在失败时重新启动它等。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;ApplicationMaster&quot;&gt;&lt;a href=&quot;#ApplicationMaster&quot; class=&quot;headerlink&quot; title=&quot;ApplicationMaster&quot;&gt;&lt;/a&gt;ApplicationMaster&lt;/h2&gt;&lt;p&gt;ApplicationMaster简称AM。YARN中每个应用都会启动一个AM。&lt;br&gt;主要功能包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;与RM调度器协商以获取资源&lt;/li&gt;
&lt;li&gt;将得到的任务进一步分配给内部的任务&lt;/li&gt;
&lt;li&gt;与NM通信以启动/停止任务&lt;/li&gt;
&lt;li&gt;监控所有任务运行状态，并在任务失败时重新为任务申请资源以重新运行任务&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;NodeManager&quot;&gt;&lt;a href=&quot;#NodeManager&quot; class=&quot;headerlink&quot; title=&quot;NodeManager&quot;&gt;&lt;/a&gt;NodeManager&lt;/h2&gt;&lt;p&gt;NodeManager简称NM，NM是每个节点上的资源和任务管理器&lt;br&gt;主要功能包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定时向RM汇报本节点上的资源使用情况和各个Continer的运行状态&lt;/li&gt;
&lt;li&gt;接收并处理来自AM的Container启动/停止等各种请求&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;Container&quot;&gt;&lt;a href=&quot;#Container&quot; class=&quot;headerlink&quot; title=&quot;Container&quot;&gt;&lt;/a&gt;Container&lt;/h2&gt;&lt;p&gt;Container是YARN中资源容器。它封装了某个节点上的多维度资源，如内存、cpu、磁盘、网络等。YARN中所有的应用都是在container上运行的。AM也是在container上运行的，不过AM的container是向RM申请的。YARN会为每一个任务分配一个Container，并且该任务只能使用该Container中描述的资源。&lt;/p&gt;
&lt;h2 id=&quot;YARN工作流程&quot;&gt;&lt;a href=&quot;#YARN工作流程&quot; class=&quot;headerlink&quot; title=&quot;YARN工作流程&quot;&gt;&lt;/a&gt;YARN工作流程&lt;/h2&gt;&lt;p&gt;YARN上运行的应用程序主要分为两类：短应用程序和长应用程序。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;短应用程序是指一定时间内（可能是秒级、分钟级或小时级，尽管天级别或者更长时间的也存在，但是非常少）可运行完成并征程退出的应用程序&lt;/li&gt;
&lt;li&gt;长应用程序是指不出意外，永不终止运行的应用程序，通常是一些服务，如Storm，HBase等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;YARN的工作流程包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;向YARN中提交一个应用程序，其中包括ApplicationMaster程序，启动ApplicationMaster的命令，用户程序等&lt;/li&gt;
&lt;li&gt;ResourceManager为该应用程序分配一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster&lt;/li&gt;
&lt;li&gt;ApplicationMaster首先向ResourceManager注册（注册之后可以直接通过ResourceManager查看应用程序的运行状态），然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束&lt;/li&gt;
&lt;li&gt;ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源&lt;/li&gt;
&lt;li&gt;一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务&lt;/li&gt;
&lt;li&gt;NodeManager为任务设置好运行环境（包括环境变量，jar包，二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务&lt;/li&gt;
&lt;li&gt;各个任务通过RPC协议向ApplicationMaster汇报自己的状态和进度，使得ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;YARN采用Master/Slave架构，ResourceManager为Master，NodeManager为Slave。ResourceManager负责对各个NodeManager上的资源进行统一管理和调度，当用户提交一个应用程序时，需要提供一个用来跟踪和管理这个程序
    
    </summary>
    
    
      <category term="hadoop" scheme="http://dxer.github.io/tags/hadoop/"/>
    
      <category term="bigdata" scheme="http://dxer.github.io/tags/bigdata/"/>
    
      <category term="YAR" scheme="http://dxer.github.io/tags/YAR/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 1.0 &amp; Hadoop 2.0</title>
    <link href="http://dxer.github.io/2015/11/03/hadoop%20v1.0%20and%20hadoopv2.0/"/>
    <id>http://dxer.github.io/2015/11/03/hadoop v1.0 and hadoopv2.0/</id>
    <published>2015-11-03T01:29:19.000Z</published>
    <updated>2016-05-16T07:06:42.164Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Hadoop-1-0&quot;&gt;&lt;a href=&quot;#Hadoop-1-0&quot; class=&quot;headerlink&quot; title=&quot;Hadoop 1.0&quot;&gt;&lt;/a&gt;Hadoop 1.0&lt;/h2&gt;&lt;p&gt;Hadoop 1.0即第一代Hadoop，由分布式存储系统HDFS和分布式和分布式计算框架MapReduce组成。其中HDFS由一个NameNode和多个DataNode组成MapReduce由一个JobTracker和多个TaskTracker组成，对应的Hadoop版本为Apache Hadoop 0.20.x、1.x、0.21.x、0.22.x和CDH3。&lt;/p&gt;
&lt;h2 id=&quot;Hadoop-2-0&quot;&gt;&lt;a href=&quot;#Hadoop-2-0&quot; class=&quot;headerlink&quot; title=&quot;Hadoop 2.0&quot;&gt;&lt;/a&gt;Hadoop 2.0&lt;/h2&gt;&lt;p&gt;Hadoop 2.0即第二代Hadoop，为了克服Hadooop 1.0中HDFS和MapReduce存在的各种问题而提出的。针对Hadoop 1.0中的单NameNode制约HDFS的扩展性问题，提出了HDFS Federation，它让多个NameNode分管不同的目录进而实现访问隔离和横向扩展，同时解决了NameNode的单点故障问题。针对Hadoop 1.0中的MapReduce在扩展性和多框架支持等方面的不足，它将JobTracker中的资源管理和作业控制功能分开，分别由组件ResourceManager和ApplicationMaster实现，其中，ResourceManager负责所有应用程序的资源分配，ApplicationMaster仅负责管理一个应用程序，着便是全新的通用的资源管理框架YARN。Hadoop 2.0对应的Hadoop的版本为Apache Hadoop 0.23.x、2.x和CDH4。&lt;/p&gt;
&lt;h2 id=&quot;MRv1&quot;&gt;&lt;a href=&quot;#MRv1&quot; class=&quot;headerlink&quot; title=&quot;MRv1&quot;&gt;&lt;/a&gt;MRv1&lt;/h2&gt;&lt;p&gt;MapReduce 1.0计算框架主要由三部分组成，分别是编程模型，数据处理引擎和运行时环境。它的编程模型是将问题抽象成Map和Reduce两个阶段，其中Map阶段将输入数据解析成key/value，迭代调用map()函数处理后，再以key/value的形式输出到本地目录，而Reduce阶段则是将Map阶段输出的数据按照key相同的value进行归约处理，并将最终结果写到HDFS上。它的数据处理引擎由MapTask和ReduceTask组成，分别负责Map阶段逻辑和Reduce阶段逻辑的处理。运行时的环境由一个JobTracker和若干个TaskTracker组成，JobTracker负责资源管理和所有作业的控制，TaskTracker负责接收来自JobTracker的命令并执行它。&lt;/p&gt;
&lt;h2 id=&quot;MRv2&quot;&gt;&lt;a href=&quot;#MRv2&quot; class=&quot;headerlink&quot; title=&quot;MRv2&quot;&gt;&lt;/a&gt;MRv2&lt;/h2&gt;&lt;p&gt;MRv2具有与MRv1相同的编程模型和数据处理引擎，唯一不同的是运行时环境。MRv2是在MRv1基础上经加工之后，运行于资源管理框架YARN之上的计算框架。它由资源管理系统YARN和作业控制进程ApplicationMaster组成，其中YARN负责资源管理和调度，而ApplicationMaster仅负责一个作业的管理。MRv1仅是一个独立的离线计算框架，而MRv2则是运行于YARN之上的MapReduce。&lt;/p&gt;
&lt;h2 id=&quot;YARN&quot;&gt;&lt;a href=&quot;#YARN&quot; class=&quot;headerlink&quot; title=&quot;YARN&quot;&gt;&lt;/a&gt;YARN&lt;/h2&gt;&lt;p&gt;YARN是Hadoop 2.0中的资源管理系统，它是一个通用的资源管理模块，可以为各类应用横须进行资源管理和调度。YARN不仅限于MapReduce一种框架使用，也可以供其他框架使用，比如Spark,Storm等。&lt;/p&gt;
&lt;h2 id=&quot;HDFS-Federation&quot;&gt;&lt;a href=&quot;#HDFS-Federation&quot; class=&quot;headerlink&quot; title=&quot;HDFS Federation&quot;&gt;&lt;/a&gt;HDFS Federation&lt;/h2&gt;&lt;p&gt;在Hadoop 2.0中，对HDFS进行改进，是的NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Hadoop-1-0&quot;&gt;&lt;a href=&quot;#Hadoop-1-0&quot; class=&quot;headerlink&quot; title=&quot;Hadoop 1.0&quot;&gt;&lt;/a&gt;Hadoop 1.0&lt;/h2&gt;&lt;p&gt;Hadoop 1.0即第一代Hadoop，由分布式存储系统HDFS和分布式和
    
    </summary>
    
    
      <category term="hadoop" scheme="http://dxer.github.io/tags/hadoop/"/>
    
      <category term="mapreduce" scheme="http://dxer.github.io/tags/mapreduce/"/>
    
      <category term="bigdata" scheme="http://dxer.github.io/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>redis学习 - 数据持久化</title>
    <link href="http://dxer.github.io/2015/05/07/redis-persistence/"/>
    <id>http://dxer.github.io/2015/05/07/redis-persistence/</id>
    <published>2015-05-07T06:55:38.000Z</published>
    <updated>2016-04-28T10:33:59.527Z</updated>
    
    <content type="html">&lt;p&gt;Redis提供了多种不同级别的持久化方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RDB 持久化可以在指定的时间间隔内产生数据集的时间点快照（point-in-time snapshot）&lt;/li&gt;
&lt;li&gt;AOF持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。AOF文件中的命令全部以Redis协议的格式来保存，新命令会被追加到文件的末尾.Redis还可以在后台对AOF文件进行重写(rewrite)，使得AOF文件的体积不会超过保存数据集状态所需的实际大小。&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Redis还可以同时使用AOF和RDB持久化。在这种情况下，当Redis重启时，它会优先使用AOF文件来还原数据集，因为AOF文件保存的数据集通常比RDB文件所保存的数据集更完整。&lt;/li&gt;
&lt;li&gt;可以关闭持久化功能，让数据只在服务器运行时存在。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;RDB的优点&quot;&gt;&lt;a href=&quot;#RDB的优点&quot; class=&quot;headerlink&quot; title=&quot;RDB的优点&quot;&gt;&lt;/a&gt;RDB的优点&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;RDB是一个很紧凑的文件，它保存了redis在某个时间点上的数据集。&lt;/li&gt;
&lt;li&gt;RDB非常适用于灾难恢复，它只有一个文件，并且内容紧凑，可以将它传送到别的数据中心&lt;/li&gt;
&lt;li&gt;可以最大化redis的性能；父进程在保存RDB文件时，唯一要做的就是fork一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无需执行任何磁盘I/O操作&lt;/li&gt;
&lt;li&gt;RDB在恢复大数据集的时候比AOF的速度要快&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;RDB的缺点&quot;&gt;&lt;a href=&quot;#RDB的缺点&quot; class=&quot;headerlink&quot; title=&quot;RDB的缺点&quot;&gt;&lt;/a&gt;RDB的缺点&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;因为RDB是保存在某个时间点上的数据集，这样的话，服务器故障可能会丢失数据。&lt;/li&gt;
&lt;li&gt;每次保存RDB的时候，redis要fork一个子进程，并由子进程来进行实际的持久化工作，在数据集比较大的时候，fork可能会非常耗时，可能会造成服务器停止处理客户端请求；如果数据集非常巨大，并且cpu比较紧张的话，那么 这种停止时间设置可能会长达整整1秒。虽然AOF重写也需要进行fork，但无论AOF重写的执行间隔有多长，数据的耐久性都不会有任何损失&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&quot;AOF优点&quot;&gt;&lt;a href=&quot;#AOF优点&quot; class=&quot;headerlink&quot; title=&quot;AOF优点&quot;&gt;&lt;/a&gt;AOF优点&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;使用AOF持久化会让redis变得非常耐久，你可以设置不同的fsync策略，比如无fsync，每秒钟一次fsync，或者每次写入命令是fsync。AOF的默认策略为每秒钟fsync一次，在这种配置下，redis仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据&lt;/li&gt;
&lt;li&gt;AOF文件只是一个日志文件追加操作（append only log），因此对AOF文件的写入不需要进行seek，即使日志因为某些原因而包含了未写入完整命令（比如写入时， 磁盘满了，写入时中途停机等），redis-check-aof工具可以轻易的修复这种问题&lt;/li&gt;
&lt;li&gt;redis可以在AOF文件体积过大时，自动在后台对AOF进行重写，重写后的AOF文件包含了恢复当前数据集所需的最小命令集合。这个重写操作是绝对安全的，因为redis在创建新的AOF过程中，会继续讲命令追加到现有的AOF文件里面，即使重写过程中发生停机，现有的AOF文件也不会丢失，而一旦新AOF文件创建完毕，redis就会从旧文件切换到新AOF文件，并开始对新AOF文件进行追加操作&lt;/li&gt;
&lt;li&gt;AOF文件有序地保存了对数据库执行的所有写入操作，这些写入操作以redis协议的格式保存，因此AOF文件的内容非常容易被人读懂，对文件进行分析也很容易。到处AOF文件也非常简单。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;AOF缺点&quot;&gt;&lt;a href=&quot;#AOF缺点&quot; class=&quot;headerlink&quot; title=&quot;AOF缺点&quot;&gt;&lt;/a&gt;AOF缺点&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;对于相同的数据来说，AOF文件的体积通常要大于RDB文件的体积&lt;/li&gt;
&lt;li&gt;根据所使用的fsync策略。AOF的速度可能会慢于RDB。&lt;/li&gt;
&lt;li&gt;AOF的bug，曾经因为个别命令的原因，导致AOF文件在重新载入是，无法将数据集恢复成保存时的样子。&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Redis提供了多种不同级别的持久化方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RDB 持久化可以在指定的时间间隔内产生数据集的时间点快照（point-in-time snapshot）&lt;/li&gt;
&lt;li&gt;AOF持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。AOF文件中的命令全部以Redis协议的格式来保存，新命令会被追加到文件的末尾.Redis还可以在后台对AOF文件进行重写(rewrite)，使得AOF文件的体积不会超过保存数据集状态所需的实际大小。
    
    </summary>
    
    
      <category term="redis" scheme="http://dxer.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis学习 - 主从拷贝</title>
    <link href="http://dxer.github.io/2015/05/07/redis_master_slave_copy/"/>
    <id>http://dxer.github.io/2015/05/07/redis_master_slave_copy/</id>
    <published>2015-05-07T06:29:19.000Z</published>
    <updated>2016-04-28T10:34:17.814Z</updated>
    
    <content type="html">&lt;h2 id=&quot;Redis主从拷贝的特点&quot;&gt;&lt;a href=&quot;#Redis主从拷贝的特点&quot; class=&quot;headerlink&quot; title=&quot;Redis主从拷贝的特点&quot;&gt;&lt;/a&gt;Redis主从拷贝的特点&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;同一个Master可以拥有多个Slaves。&lt;/li&gt;
&lt;li&gt;Master下的Slave还可以接受同一架构中其它slave的链接与同步请求，实现数据的级联复制，即Master-&amp;gt;Slave-&amp;gt;Slave模式；&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Master以非阻塞的方式同步数据至slave，这将意味着Master会继续处理一个或多个slave的读写请求；&lt;br&gt;4.Slave端同步数据也可以修改为非阻塞是的方式，当slave在执行新的同步时，它仍可以用旧的数据信息来提供查询；否则，当slave与master失去联系时，slave会返回一个错误给客户端；&lt;/li&gt;
&lt;li&gt;主从复制具有可扩展性，即多个slave专门提供只读查询与数据的冗余，Master端专门提供写操作；&lt;/li&gt;
&lt;li&gt;通过配置禁用Master数据持久化机制，将其数据持久化操作交给Slaves完成，避免在Master中要有独立的进程来完成此操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;Redis主从拷贝的过程&quot;&gt;&lt;a href=&quot;#Redis主从拷贝的过程&quot; class=&quot;headerlink&quot; title=&quot;Redis主从拷贝的过程&quot;&gt;&lt;/a&gt;Redis主从拷贝的过程&lt;/h2&gt;&lt;p&gt;slave连接上master之后，slave发送一个SYNC命令到master，master接收到命令之后，无论是第一次同步建立的连接，还是连接断开后的重新连接，master会开启BGSAVE操作，启动一个后台进程，保存一份当前master内存快照，并且开始保存从调用BGSAVE之后的所有写命令，master生成完快照之后，发送内存快照rdb文件给slave。slave接收到master发送过来的rdb文件之后，将清空所有旧数据，加载接收到的rdb文件到内存中，发送完rdb文件给slave之后，开始发送刚刚保存的写操作日志给slave，slave执行这些写操作，至此，主从数据保存一致。发送完写日志之后，master会增量发送之后的写操作给slave，使主从一致。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ps: 当master和slave的连接断开时，slave可以自动重新建立连接。如果master同时收到多个slave发来的同步连接命令，只会使用启动一个进程来写内存快照，然后发送给所有的slave&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h6 id=&quot;Master-write-Slave-read机制&quot;&gt;&lt;a href=&quot;#Master-write-Slave-read机制&quot; class=&quot;headerlink&quot; title=&quot;Master write, Slave read机制&quot;&gt;&lt;/a&gt;Master write, Slave read机制&lt;/h6&gt;&lt;p&gt;redis的主从复制，通过程序实现数据的读写分离，让master负责处理些请求，slave负责处理读请求，通过扩展slave处理更多的并发请求，减轻master端的负载。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ps:在程序中判断用户的读写请求，将write请求发送给master，read请求发送给slave处理&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;redis主从拷贝配置&quot;&gt;&lt;a href=&quot;#redis主从拷贝配置&quot; class=&quot;headerlink&quot; title=&quot;redis主从拷贝配置&quot;&gt;&lt;/a&gt;redis主从拷贝配置&lt;/h2&gt;&lt;p&gt;开启主从复制，最简单的方式，连接上从机redis，执行slaveof &amp;lt;主机host&amp;gt; &amp;lt;主机端口&amp;gt;，另外也可以在从机的配置文件中加入slaveof &amp;lt;主机host&amp;gt; &amp;lt;主机端口&amp;gt;，这样从机启动的时候，就会自动连接主机，并且同步数据。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;slaveof 192.168.100.126 6379 # 配置主机信息&lt;br&gt;masterauth &lt;master-password&gt; # 如果主机设置了密码，配置密码&lt;br&gt;slave-serve-stale-data yes # 配置当从机正在和主机进行同步的时候是否响应，如果配置是，有可能客户端会读到旧数据，如果配置否，当请求读数据的时候，将会报错SYNC with master in progress&lt;br&gt;slave-read-only yes # 从机是否只读。这边设置可写，不会同步到主机，&lt;br&gt;repl-ping-slave-period 10 # 从机发送ping命令到主机的间隔时间。&lt;br&gt;repl-timeout 60 # 主机响应超时时间，这个包括传输超时，IO超时，ping超时，注意这边时间必须大于上面的间隔时间，要不然会一直报超时错误。&lt;br&gt;repl-disable-tcp-nodelay no # 是否禁用TCP NODELAY。官方对这个配置用法的建议是：&lt;br&gt;# By default we optimize for low latency, but in very high traffic conditions&lt;br&gt;# or when the master and slaves are many hops away, turning this to “yes” may&lt;br&gt;# be a good idea.&lt;br&gt;# 默认情况下，我们优化目的是为了低延迟，但是在高传输条件或者主从机分布在路由很多跳之外的，建议禁用掉tcp-nodelay。&lt;br&gt;slave-priority 100 # 如果master不能再正常工作，那么会在多个slave中，选择优先值最小的一个slave提升为master，优先值为0表示不能提升为master&lt;/master-password&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Redis主从拷贝的特点&quot;&gt;&lt;a href=&quot;#Redis主从拷贝的特点&quot; class=&quot;headerlink&quot; title=&quot;Redis主从拷贝的特点&quot;&gt;&lt;/a&gt;Redis主从拷贝的特点&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;同一个Master可以拥有多个Slaves。&lt;/li&gt;
&lt;li&gt;Master下的Slave还可以接受同一架构中其它slave的链接与同步请求，实现数据的级联复制，即Master-&amp;gt;Slave-&amp;gt;Slave模式；
    
    </summary>
    
    
      <category term="redis" scheme="http://dxer.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>cookie与session</title>
    <link href="http://dxer.github.io/2015/04/17/cookie_and_session/"/>
    <id>http://dxer.github.io/2015/04/17/cookie_and_session/</id>
    <published>2015-04-17T02:43:19.000Z</published>
    <updated>2016-05-16T07:57:36.106Z</updated>
    
    <content type="html">&lt;p&gt;会话（session）跟踪式web程序中常用的技术，用来跟踪用户的整个会话。常用的会话跟踪技术是cookie与session。coolie是通过在客户端记录信息确定用户身份，session是通过在服务端记录信息确定用户身份。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;cookie&quot;&gt;&lt;a href=&quot;#cookie&quot; class=&quot;headerlink&quot; title=&quot;cookie&quot;&gt;&lt;/a&gt;cookie&lt;/h2&gt;&lt;p&gt;cookie技术是客户端解决方案。cookie是由服务器发给客户端的特殊信息，这些信息以文本的方式存放在客户端，客户端每次向服务器发送请求的时候都会带上这些特殊的信息。&lt;/p&gt;
&lt;p&gt;例如：在用户访问一个网站的时候，先登录，用户将用户名和密码发送给服务器；服务器对用户名和密码进行验证，验证成功后，会向客户端回传相应的超文本信息的同时也是返回这些个人信息，这些个人信息存放在HTTP的头部；当客户端接收到服务端的响应之后，浏览器会将这些个人信息存放在一个统一的位置；后面如果客户端再向服务端请求的时候，都会把cookie信息发回到服务器。服务器在接收到来自客户端的请求之后，分析存放于请求头的cookie信息，得到用户相关信息，从而生成与该客户端请求的内容。&lt;/p&gt;
&lt;p&gt;cookie是弥补HTTP协议的无状态的不足。&lt;/p&gt;
&lt;p&gt;cookies是HTTP的一个扩展，有两个专门负责设置以及发送cookie的http头部，Set-Cookie和Cookie。当服务器返回给客户端一个http响应时，如果包含Set-Cookie这个头部时，指示客户端建立一个新的cookie，并且在后续的http请求中自动发送这个cookie到服务端，直到这个cookie过期。如果cookie的生存时间是整个会话期间，浏览器会将cookie保存在内存中个，在浏览器关闭的时候会自动清除这个cookie。cookie也可以保存在客户端的磁盘上，这种情况，在浏览器关闭之后，该cookie也不会清除，下次打开浏览器访问对应网站的时候，这个cookie就会自动再次发送到服务器端。&lt;/p&gt;
&lt;p&gt;客户端发送一个http请求到服务器端，服务器端发送一个http响应到客户端，其中包含Set-Cookie头部&lt;/p&gt;
&lt;p&gt;客户端发送一个http请求到服务器端，其中包含Cookie头部&lt;/p&gt;
&lt;h3 id=&quot;cookie的不可跨域性&quot;&gt;&lt;a href=&quot;#cookie的不可跨域性&quot; class=&quot;headerlink&quot; title=&quot;cookie的不可跨域性&quot;&gt;&lt;/a&gt;cookie的不可跨域性&lt;/h3&gt;&lt;p&gt;很多网站都会使用cookie，但是不同网站的只能使用本网站生成的cookie，不能进行跨域访问。&lt;/p&gt;
&lt;p&gt;cookie中使用Unicode字符时，需要对Unicode字符进行编码，否则会乱码&lt;/p&gt;
&lt;p&gt;cookie不仅可以使用ASCII字符与Unicode字符，还可以使用二进制数据，&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;session&quot;&gt;&lt;a href=&quot;#session&quot; class=&quot;headerlink&quot; title=&quot;session&quot;&gt;&lt;/a&gt;session&lt;/h2&gt;&lt;p&gt;web应用中还经常使用session来记录客户端的状态，session是服务器端使用的一种记录客户端状态的机制，比cookie简单，但是会增加服务器的存储压力&lt;/p&gt;
&lt;p&gt;session技术是服务器端的解决方案，它是通过服务器来保持状态的，客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上，这个就是session。客户端浏览器再次访问时只需要从该session中查找客户的章台就可以了。&lt;/p&gt;
&lt;p&gt;session类似在服务器上建立了一个客户档案，客户来访的时候只需要查询客户档案就可以了。&lt;/p&gt;
&lt;h3 id=&quot;session的生命周期&quot;&gt;&lt;a href=&quot;#session的生命周期&quot; class=&quot;headerlink&quot; title=&quot;session的生命周期&quot;&gt;&lt;/a&gt;session的生命周期&lt;/h3&gt;&lt;p&gt;session是保存在服务端的，为了获得更高的存取速度，服务器一般把session放在内存中，每个用户都会拥有一个独立的session。如果session的内容太复杂，当亮亮客户访问的时候，会导致服务器内存溢出。所以session里的信息应当尽量精简。&lt;/p&gt;
&lt;p&gt;session在用户第一次访问服务器的时候自动创建，访问服务器端的静态资源是不会产生session的&lt;/p&gt;
&lt;p&gt;session生成后，用户只要继续访问，服务器就会更新session的最后访问时间，并维护该sessin，用户没访问服务器一次，无论是否读写session，服务器都认为该用户的session“活跃”了一次&lt;/p&gt;
&lt;h3 id=&quot;session的有效期&quot;&gt;&lt;a href=&quot;#session的有效期&quot; class=&quot;headerlink&quot; title=&quot;session的有效期&quot;&gt;&lt;/a&gt;session的有效期&lt;/h3&gt;&lt;p&gt;由于访问服务端的用户会越来越多，因此session也会越来越多，为了防止内存溢出，如果超过了超时时间也没有访问服务器端，该session就会自动失效了。&lt;/p&gt;
&lt;p&gt;session对浏览器的要求&lt;/p&gt;
&lt;p&gt;虽然session保存在服务器端，对客户端是透明的，它的正常运行任然需要客户端浏览器的支持，因为session需要使用cookie作为识别标志。HTTP协议是无状态的，session不能通过HTTP链接来判断是否是同一个用户，因此服务器向客户端浏览器发送一个名为JSESSIONID的cookie，它的值为该Session的id。&lt;/p&gt;
&lt;p&gt;该cookie是服务器端自动生成的，它的maxAge属性为-1，表示仅当前浏览器内有效，并且个浏览器窗口见不共享，关闭浏览器就会消失。&lt;/p&gt;
&lt;p&gt;同一台机器的两个浏览器窗口访问服务器时，会生成两个不同的session，但是有浏览器窗口内的链接，脚本等打开的新窗口除外。这类子窗口会共享父窗口的cookie，因此会共享一个session。&lt;/p&gt;
&lt;p&gt;新打开的浏览器窗口会生成新的session，但子窗口除外，子窗口会共享父窗口的session。&lt;/p&gt;
&lt;p&gt;如果客户端浏览器将cookie禁用，或者不支持cookie怎么办（绝大数手机浏览器不支持cookie）？&lt;/p&gt;
&lt;p&gt;URL地址重写&lt;/p&gt;
&lt;h2 id=&quot;URL重写&quot;&gt;&lt;a href=&quot;#URL重写&quot; class=&quot;headerlink&quot; title=&quot;URL重写&quot;&gt;&lt;/a&gt;URL重写&lt;/h2&gt;&lt;p&gt;URL地址重写是对客户端不支持cookie的解决方案&lt;/p&gt;
&lt;p&gt;URL地址重写的原理就是将用户的session的id信息重写到URL地址中，服务器能够够解析重写后的URL，从而获取session的id。这样即使客户端不支持cookie，也可以使用session来记录用户的状态了。&lt;/p&gt;
&lt;h2 id=&quot;cookie与session的区别&quot;&gt;&lt;a href=&quot;#cookie与session的区别&quot; class=&quot;headerlink&quot; title=&quot;cookie与session的区别&quot;&gt;&lt;/a&gt;cookie与session的区别&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;cookie数据存在客户端的浏览器上，session数据存放在服务器上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当那个使用session&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;session会在一定时间内保存在服务器上，当访问量增多，会占用比较多的服务器资源，考虑到减轻服务器性能方面，应当使用cookie&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;的那个cookie在客户端的限制是3k，就是说一个站点在客户端存放的cookie不能超过3k&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;会话（session）跟踪式web程序中常用的技术，用来跟踪用户的整个会话。常用的会话跟踪技术是cookie与session。coolie是通过在客户端记录信息确定用户身份，session是通过在服务端记录信息确定用户身份。&lt;br&gt;
    
    </summary>
    
    
      <category term="session" scheme="http://dxer.github.io/tags/session/"/>
    
      <category term="cookie" scheme="http://dxer.github.io/tags/cookie/"/>
    
  </entry>
  
  <entry>
    <title>深入了解HashMap</title>
    <link href="http://dxer.github.io/2015/03/21/hashmap/"/>
    <id>http://dxer.github.io/2015/03/21/hashmap/</id>
    <published>2015-03-21T07:43:19.000Z</published>
    <updated>2016-05-16T07:05:38.980Z</updated>
    
    <content type="html">&lt;p&gt;HashMap基于哈希表的Map接口的实现。此实现提供了所有可选的映射操作，允许使用null键和值。与HashTable相比，HashMap不支持同步，不是线程安全，允许使用null键值，其他大致相同。HashTable不保证映射的顺序，不保证顺序很久不变&lt;/p&gt;
&lt;p&gt;HashMap不是线程安全的，如果想要线程安全的HashMap，可以通过Collections&lt;br&gt;类的静态方法synchronizedMap获得线程安全的HashMap。HashTable是线程安全的，通过synchronized实现。&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Map map = Collections.synchronizedMap(&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; HashMap());&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;HashMap的数据结构&quot;&gt;&lt;a href=&quot;#HashMap的数据结构&quot; class=&quot;headerlink&quot; title=&quot;HashMap的数据结构&quot;&gt;&lt;/a&gt;HashMap的数据结构&lt;/h2&gt;&lt;p&gt;HashMap的底层主要是基于数组和链表来实现的，它之所以查询速度特别快，主要是因为它是通过计算散列码来决定存储的位置。&lt;/p&gt;
&lt;p&gt;从上图可以看出，x轴方向是链表的存储方式，y轴是数组。&lt;/p&gt;
&lt;h2 id=&quot;HashMap在put和get的工作流程&quot;&gt;&lt;a href=&quot;#HashMap在put和get的工作流程&quot; class=&quot;headerlink&quot; title=&quot;HashMap在put和get的工作流程&quot;&gt;&lt;/a&gt;HashMap在put和get的工作流程&lt;/h2&gt;</content>
    
    <summary type="html">
    
      &lt;p&gt;HashMap基于哈希表的Map接口的实现。此实现提供了所有可选的映射操作，允许使用null键和值。与HashTable相比，HashMap不支持同步，不是线程安全，允许使用null键值，其他大致相同。HashTable不保证映射的顺序，不保证顺序很久不变&lt;/p&gt;
&lt;p&gt;H
    
    </summary>
    
    
      <category term="java" scheme="http://dxer.github.io/tags/java/"/>
    
      <category term="hashmap" scheme="http://dxer.github.io/tags/hashmap/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 6.3忘记root密码的解决办法</title>
    <link href="http://dxer.github.io/2015/02/26/centos-passwd/"/>
    <id>http://dxer.github.io/2015/02/26/centos-passwd/</id>
    <published>2015-02-26T13:53:34.000Z</published>
    <updated>2015-05-05T06:07:42.755Z</updated>
    
    <content type="html">&lt;p&gt;忘记linux系统的登陆密码了，悲剧了，查了好多资料，终于搞定了，来和大家分享下具体操作。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;在开机启动的时候按键盘上的“E”键会进入如下界面（我的系统是centos 6.3，其他发行版不一定适用哦）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/pic/20150226/1.jpg&quot; alt=&quot;&quot;&gt;  &lt;/p&gt;
&lt;p&gt;注：本机是新装的系统，没有升级之类的操作，故选项只有一个，可能是因为只有一个选项的关系，所以需要按一下“E”键才会出现如上界面。多个应该可以直接用方向键直接选择&lt;/p&gt;
&lt;p&gt;选择相应的内核，再次按“E”，出现下图，选择第二项，再次按“E”键&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/pic/20150226/2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;经过第二步，这个画面可以编辑，在信息的最后加“空格”，然后键入“single”（如图），或者直接输入数字的“1”并回车确定进入下一步。图如下： &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/pic/20150226/3.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;操作完第三步，会出现下图，是不是感觉又回到第二步了呢？并不是，这里按键盘的”B”键，进入引导系统。注意，这儿是“B”键&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/pic/20150226/4.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在下面这个画面中的“#”后输入“passwd root”，重新设置root的密码，密码输入一遍，确认输入一遍，共2遍。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/pic/20150226/5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;注：重置密码成功会有一个修改成功的提示，然后输入reboot重启系统，root密码重置就完成了。&lt;/p&gt;
&lt;p&gt;ok，大功告成。。。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      你对本页的描述
    
    </summary>
    
    
      <category term="linux" scheme="http://dxer.github.io/tags/linux/"/>
    
      <category term="centos" scheme="http://dxer.github.io/tags/centos/"/>
    
  </entry>
  
</feed>
